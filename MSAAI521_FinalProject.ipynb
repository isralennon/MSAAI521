{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93dfaaee",
      "metadata": {
        "id": "93dfaaee"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3X3wFliA92kb",
      "metadata": {
        "id": "3X3wFliA92kb"
      },
      "outputs": [],
      "source": [
        "#Uncomment the code below if needed to install the required dependencies\n",
        "#!pip install nuscenes-devkit\n",
        "#!pip install scipy==1.5.0 scikit-learn==1.0.2 --force-reinstall --no-deps\n",
        "#!pip install -U ultralytics\n",
        "#!pip install Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "VZHFus7vbUdO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZHFus7vbUdO",
        "outputId": "c4292a69-005d-437c-85be-1a54090cf832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "h_WQ_a8Ysjlk",
      "metadata": {
        "id": "h_WQ_a8Ysjlk"
      },
      "outputs": [],
      "source": [
        "#Global Varible definitions\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build\")\n",
        "sys.path.insert(0, \"build\")\n",
        "\n",
        "BUILD_ROOT = \"/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build\"\n",
        "\n",
        "DATA_ROOT = f\"{BUILD_ROOT}/data\"\n",
        "NUSCENES_ROOT = f\"{DATA_ROOT}/raw/v1.0-mini\"\n",
        "NUSCENES_VERSION = \"v1.0-mini\"\n",
        "\n",
        "PREPROCESSED_ROOT = f\"{DATA_ROOT}/preprocessed\"\n",
        "YOLO_BEV_ROOT = f\"{DATA_ROOT}/yolo_bev\"\n",
        "\n",
        "MODELS_ROOT = f\"{BUILD_ROOT}/models\"\n",
        "\n",
        "RUNS_ROOT = f\"{BUILD_ROOT}/runs\"\n",
        "RESULTS_ROOT = f\"{BUILD_ROOT}/results\"\n",
        "\n",
        "VISUALIZATIONS_ROOT = f\"{BUILD_ROOT}/visualizations\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "QXCnVyi4rLTB",
      "metadata": {
        "id": "QXCnVyi4rLTB"
      },
      "outputs": [],
      "source": [
        "#Creating the Downloader Class\n",
        "from pathlib import Path\n",
        "#from Globals import NUSCENES_ROOT\n",
        "\n",
        "\n",
        "class DataDownloader:\n",
        "    def __init__(self, root_path=NUSCENES_ROOT):\n",
        "        self.root = Path(root_path)\n",
        "\n",
        "    def check_and_prompt(self):\n",
        "        if self.root.exists():\n",
        "            return True\n",
        "\n",
        "        print(f\"nuScenes dataset not found at: {self.root}\")\n",
        "        print()\n",
        "        print(\"Download instructions:\")\n",
        "        print(\"1. Visit: https://www.nuscenes.org/nuscenes#download\")\n",
        "        print(\"2. Download v1.0-mini (4 GB)\")\n",
        "        print(f\"3. Extract to: {self.root}\")\n",
        "        print()\n",
        "\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "FFprjgJ6wSgM",
      "metadata": {
        "id": "FFprjgJ6wSgM"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "#from Globals import NUSCENES_ROOT, NUSCENES_VERSION\n",
        "\n",
        "\n",
        "class DataValidator:\n",
        "    def __init__(self, root_path=NUSCENES_ROOT, version=NUSCENES_VERSION):\n",
        "        self.root = Path(root_path)\n",
        "        self.version = version\n",
        "\n",
        "        self.required_dirs = [\n",
        "            f'samples/LIDAR_TOP',\n",
        "            f'sweeps/LIDAR_TOP',\n",
        "            self.version\n",
        "        ]\n",
        "\n",
        "        self.required_files = [\n",
        "            f'{self.version}/sample.json',\n",
        "            f'{self.version}/sample_data.json',\n",
        "            f'{self.version}/sample_annotation.json',\n",
        "            f'{self.version}/ego_pose.json',\n",
        "            f'{self.version}/calibrated_sensor.json',\n",
        "            f'{self.version}/scene.json',\n",
        "            f'{self.version}/instance.json',\n",
        "            f'{self.version}/category.json'\n",
        "        ]\n",
        "\n",
        "    def validate(self):\n",
        "        if not self.root.exists():\n",
        "            return False\n",
        "\n",
        "        for dir_path in self.required_dirs:\n",
        "            if not (self.root / dir_path).exists():\n",
        "                return False\n",
        "\n",
        "        for file_path in self.required_files:\n",
        "            if not (self.root / file_path).exists():\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "DUKvEVTgwZBn",
      "metadata": {
        "id": "DUKvEVTgwZBn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "RawDataInspector: A comprehensive tool for exploring and visualizing nuScenes autonomous driving dataset.\n",
        "\n",
        "This class provides methods to inspect, analyze, and visualize various components of the nuScenes dataset\n",
        "including LiDAR point clouds, 3D bounding box annotations, camera images, and scene metadata.\n",
        "\n",
        "Technical Overview:\n",
        "- Interfaces with nuScenes API to access dataset metadata and sensor data\n",
        "- Processes LiDAR point clouds stored in binary .pcd.bin format\n",
        "- Handles 3D coordinate transformations and spatial data visualization\n",
        "- Generates matplotlib-based visualizations of multi-sensor autonomous vehicle data\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from nuscenes.utils.data_classes import LidarPointCloud\n",
        "from nuscenes.utils.data_classes import Box\n",
        "from nuscenes.utils.geometry_utils import BoxVisibility\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "\n",
        "class RawDataInspector:\n",
        "    \"\"\"\n",
        "    Inspector class for analyzing raw nuScenes dataset components.\n",
        "\n",
        "    This class provides a suite of methods to explore autonomous vehicle sensor data,\n",
        "    including LiDAR point clouds, camera images, and 3D object annotations. All visualizations\n",
        "    are saved to disk rather than displayed interactively for compatibility with headless environments.\n",
        "\n",
        "    Attributes:\n",
        "        nusc: NuScenes instance providing access to the dataset API\n",
        "        output_dir: Path object pointing to directory where visualizations are saved\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nusc, output_dir='build/visualizations'):\n",
        "        \"\"\"\n",
        "        Initialize the RawDataInspector with a nuScenes dataset instance.\n",
        "\n",
        "        Args:\n",
        "            nusc: A NuScenes instance that provides API access to the dataset.\n",
        "                  This object contains all the metadata tables (scenes, samples, annotations, etc.)\n",
        "                  and methods to query and render the dataset.\n",
        "            output_dir: String path where visualization files will be saved.\n",
        "                       Defaults to 'build/visualizations'. Directory is created if it doesn't exist.\n",
        "\n",
        "        Technical Details:\n",
        "            - Creates output directory structure using pathlib for cross-platform compatibility\n",
        "            - Stores nuScenes instance for accessing dataset metadata tables and file paths\n",
        "        \"\"\"\n",
        "        self.nusc = nusc\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "    def inspect_point_cloud(self, sample_data_token):\n",
        "        \"\"\"\n",
        "        Extract and analyze statistics from a LiDAR point cloud.\n",
        "\n",
        "        This method loads a LiDAR point cloud file and computes spatial and intensity statistics\n",
        "        to understand the point cloud's coverage and characteristics.\n",
        "\n",
        "        Args:\n",
        "            sample_data_token: String UUID identifying a specific LiDAR sensor capture in the dataset.\n",
        "                              This token references an entry in the sample_data table.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing:\n",
        "                - shape: Tuple (4, N) where N is number of points. First 3 rows are x,y,z coordinates,\n",
        "                        4th row is intensity values\n",
        "                - num_points: Total number of 3D points captured by the LiDAR\n",
        "                - x_range: Tuple (min, max) of x-coordinates in meters (forward/backward)\n",
        "                - y_range: Tuple (min, max) of y-coordinates in meters (left/right)\n",
        "                - z_range: Tuple (min, max) of z-coordinates in meters (up/down)\n",
        "                - intensity_range: Tuple (min, max) of LiDAR return intensity values\n",
        "\n",
        "        Technical Details:\n",
        "            1. Queries nuScenes metadata to get file path for the LiDAR scan\n",
        "            2. Loads binary point cloud file (.pcd.bin format) using nuScenes utilities\n",
        "            3. Point cloud data structure: 4xN numpy array where:\n",
        "               - Row 0: X coordinates (forward direction in vehicle frame)\n",
        "               - Row 1: Y coordinates (left direction in vehicle frame)\n",
        "               - Row 2: Z coordinates (up direction in vehicle frame)\n",
        "               - Row 3: Intensity values (reflectivity of laser return)\n",
        "            4. Computes min/max statistics along each dimension using numpy operations\n",
        "        \"\"\"\n",
        "        # Retrieve metadata record for this LiDAR capture from sample_data table\n",
        "        sample_data = self.nusc.get('sample_data', sample_data_token)\n",
        "\n",
        "        # Construct absolute file path to the binary point cloud file\n",
        "        pcl_path = os.path.join(self.nusc.dataroot, sample_data['filename'])\n",
        "\n",
        "        # Load point cloud from binary file into LidarPointCloud object\n",
        "        # File format: binary float32 array with 4 values per point (x, y, z, intensity)\n",
        "        pc = LidarPointCloud.from_file(pcl_path)\n",
        "        points = pc.points  # Access underlying numpy array (4, N)\n",
        "\n",
        "        # Compute and return statistical summary of point cloud\n",
        "        return {\n",
        "            'shape': points.shape,  # (4, N) - 4 channels, N points\n",
        "            'num_points': points.shape[1],  # Total number of points\n",
        "            'x_range': (points[0].min(), points[0].max()),  # Forward/backward extent (meters)\n",
        "            'y_range': (points[1].min(), points[1].max()),  # Left/right extent (meters)\n",
        "            'z_range': (points[2].min(), points[2].max()),  # Up/down extent (meters)\n",
        "            'intensity_range': (points[3].min(), points[3].max())  # LiDAR intensity values\n",
        "        }\n",
        "\n",
        "    def inspect_annotations(self, sample_token):\n",
        "        \"\"\"\n",
        "        Extract all 3D bounding box annotations for a given sample (timestamp).\n",
        "\n",
        "        Each sample in nuScenes represents a synchronized snapshot from all sensors at a specific\n",
        "        timestamp. This method retrieves all object annotations (3D bounding boxes) associated\n",
        "        with that sample.\n",
        "\n",
        "        Args:\n",
        "            sample_token: String UUID identifying a sample (multi-sensor snapshot at one timestamp).\n",
        "                         References an entry in the sample table.\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries, one per annotated object, each containing:\n",
        "                - category: String object class (e.g., 'vehicle.car', 'human.pedestrian.adult')\n",
        "                - translation: [x, y, z] center of 3D bounding box in global coordinates (meters)\n",
        "                - size: [width, length, height] dimensions of bounding box (meters)\n",
        "                - rotation: Quaternion [w, x, y, z] representing 3D orientation of the box\n",
        "\n",
        "        Technical Details:\n",
        "            1. Queries sample table to get list of annotation tokens for this timestamp\n",
        "            2. Each sample contains 'anns' field: list of annotation token UUIDs\n",
        "            3. For each annotation token, queries sample_annotation table for full metadata\n",
        "            4. Coordinate system: Global world coordinates (not ego vehicle frame)\n",
        "            5. Rotation format: Quaternion for 3D rotation (avoids gimbal lock issues)\n",
        "            6. Size convention: [width (left-right), length (forward-back), height (up-down)]\n",
        "        \"\"\"\n",
        "        # Retrieve the sample record containing list of annotation tokens\n",
        "        sample = self.nusc.get('sample', sample_token)\n",
        "\n",
        "        # Collect detailed annotation data for each object in this sample\n",
        "        annotations = []\n",
        "        for ann_token in sample['anns']:  # Iterate over all annotation UUIDs\n",
        "            # Query sample_annotation table for full annotation metadata\n",
        "            ann = self.nusc.get('sample_annotation', ann_token)\n",
        "\n",
        "            # Extract key 3D bounding box parameters\n",
        "            annotations.append({\n",
        "                'category': ann['category_name'],  # Object class label\n",
        "                'translation': ann['translation'],  # 3D position: [x, y, z] in global frame (meters)\n",
        "                'size': ann['size'],  # Box dimensions: [width, length, height] (meters)\n",
        "                'rotation': ann['rotation']  # Orientation: quaternion [w, x, y, z]\n",
        "            })\n",
        "\n",
        "        return annotations\n",
        "\n",
        "    def visualize_3d_scene(self, sample_token):\n",
        "        \"\"\"\n",
        "        Create a 3D visualization of LiDAR point cloud with overlaid 3D bounding box annotations.\n",
        "\n",
        "        This method generates a 3D scatter plot showing the spatial distribution of LiDAR points\n",
        "        colored by height (z-coordinate), with red wireframe boxes representing annotated objects.\n",
        "        The visualization is saved as a PNG file for inspection.\n",
        "\n",
        "        Args:\n",
        "            sample_token: String UUID identifying the sample (timestamp) to visualize.\n",
        "\n",
        "        Technical Details:\n",
        "            1. Data Loading:\n",
        "               - Retrieves LIDAR_TOP sensor data token from the sample\n",
        "               - Loads binary point cloud file and extracts xyz coordinates (discards intensity)\n",
        "\n",
        "            2. Point Subsampling:\n",
        "               - Randomly samples up to 10,000 points for performance (full clouds have ~30K points)\n",
        "               - Uses numpy.random.choice with replace=False for uniform random sampling\n",
        "               - Reduces rendering time while maintaining spatial distribution\n",
        "\n",
        "            3. 3D Scatter Plot:\n",
        "               - Creates matplotlib 3D axis using projection='3d'\n",
        "               - Points colored by z-coordinate (height) using 'viridis' colormap\n",
        "               - Small point size (s=0.1) and transparency (alpha=0.5) for better visibility\n",
        "\n",
        "            4. Bounding Box Overlay:\n",
        "               - Retrieves 3D boxes transformed to sensor coordinate frame\n",
        "               - BoxVisibility.ANY includes all boxes regardless of visibility status\n",
        "               - Each box rendered as wireframe by connecting bottom face corners\n",
        "               - Corners format: 3x8 array (xyz coordinates of 8 box vertices)\n",
        "               - Draws 4 edges connecting bottom face corners (indices 0,1,2,3)\n",
        "\n",
        "            5. Coordinate System:\n",
        "               - X-axis: Forward direction (vehicle's driving direction)\n",
        "               - Y-axis: Left direction (perpendicular to driving direction)\n",
        "               - Z-axis: Up direction (vertical, perpendicular to ground)\n",
        "\n",
        "            6. Output:\n",
        "               - Saves high-resolution PNG (150 DPI) with tight bounding box\n",
        "               - Closes figure to free memory (important in batch processing)\n",
        "        \"\"\"\n",
        "        # Get the sample record and extract LIDAR_TOP sensor token\n",
        "        sample = self.nusc.get('sample', sample_token)\n",
        "        lidar_token = sample['data']['LIDAR_TOP']  # Top-mounted LiDAR is primary 3D sensor\n",
        "\n",
        "        # Load point cloud file from disk\n",
        "        sample_data = self.nusc.get('sample_data', lidar_token)\n",
        "        pcl_path = os.path.join(self.nusc.dataroot, sample_data['filename'])\n",
        "        pc = LidarPointCloud.from_file(pcl_path)\n",
        "        points = pc.points[:3, :]  # Extract only xyz coordinates (discard intensity channel)\n",
        "\n",
        "        # Subsample points for efficient rendering (typical cloud has ~30K-40K points)\n",
        "        indices = np.random.choice(points.shape[1],  # Total available points\n",
        "                                   size=min(10000, points.shape[1]),  # Sample up to 10K\n",
        "                                   replace=False)  # No duplicates\n",
        "        points_sampled = points[:, indices]\n",
        "\n",
        "        # Create 3D matplotlib figure and axis\n",
        "        fig = plt.figure(figsize=(12, 8))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        # Render point cloud as 3D scatter plot\n",
        "        # Color points by height (z-coordinate) for depth perception\n",
        "        ax.scatter(points_sampled[0],  # X coordinates\n",
        "                   points_sampled[1],  # Y coordinates\n",
        "                   points_sampled[2],  # Z coordinates\n",
        "                   c=points_sampled[2],  # Color by height\n",
        "                   cmap='viridis',  # Yellow (high) to purple (low) colormap\n",
        "                   s=0.1,  # Small point size\n",
        "                   alpha=0.5)  # Semi-transparent for better overlap visibility\n",
        "\n",
        "        # Retrieve 3D bounding boxes transformed to LiDAR sensor frame\n",
        "        # Returns: pointcloud, boxes (in sensor frame), camera_intrinsic\n",
        "        _, boxes, _ = self.nusc.get_sample_data(lidar_token,\n",
        "                                                box_vis_level=BoxVisibility.ANY)\n",
        "\n",
        "        # Draw wireframe bounding boxes in red\n",
        "        for box in boxes:\n",
        "            corners = box.corners()  # Get 3x8 array of box corner coordinates\n",
        "\n",
        "            # Draw bottom face of bounding box (4 edges connecting corners 0,1,2,3)\n",
        "            for i in [0, 1, 2, 3]:\n",
        "                j = (i + 1) % 4  # Next corner (wraps 3->0)\n",
        "                ax.plot([corners[0, i], corners[0, j]],  # X coordinates of edge\n",
        "                        [corners[1, i], corners[1, j]],  # Y coordinates of edge\n",
        "                        [corners[2, i], corners[2, j]],  # Z coordinates of edge\n",
        "                        'r-', linewidth=2)  # Red solid line\n",
        "\n",
        "        # Set axis labels with units\n",
        "        ax.set_xlabel('X (m)')  # Forward\n",
        "        ax.set_ylabel('Y (m)')  # Left\n",
        "        ax.set_zlabel('Z (m)')  # Up\n",
        "        ax.set_title('3D LiDAR Scene with Annotations')\n",
        "\n",
        "        # Save figure to disk and clean up\n",
        "        output_path = self.output_dir / f'3d_scene_{sample_token}.png'\n",
        "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()  # Free memory\n",
        "        print(f\"Saved 3D scene visualization to: {output_path}\")\n",
        "\n",
        "    def list_scenes(self):\n",
        "        \"\"\"\n",
        "        Print a formatted list of all scenes in the dataset to console.\n",
        "\n",
        "        A 'scene' in nuScenes represents a continuous 20-second driving segment.\n",
        "        This method delegates to nuScenes' built-in list_scenes() which prints:\n",
        "        - Scene token (unique identifier)\n",
        "        - Scene name/description\n",
        "        - Timestamp when recorded\n",
        "        - Duration in seconds\n",
        "        - Location (e.g., boston-seaport, singapore-onenorth)\n",
        "        - Number of annotations in the scene\n",
        "\n",
        "        Technical Details:\n",
        "            - Uses nuScenes API's formatted output\n",
        "            - Useful for browsing dataset contents and selecting scenes for analysis\n",
        "            - v1.0-mini contains 10 scenes (subset of full dataset's 1000 scenes)\n",
        "        \"\"\"\n",
        "        self.nusc.list_scenes()\n",
        "\n",
        "    def visualize_sample(self):\n",
        "        \"\"\"\n",
        "        Render a multi-sensor visualization of a complete sample (all 6 cameras).\n",
        "\n",
        "        Creates a 2x3 grid showing synchronized images from all 6 cameras mounted on the vehicle\n",
        "        at a single timestamp. This provides a 360-degree view around the autonomous vehicle.\n",
        "\n",
        "        Technical Details:\n",
        "            1. Hardcoded to visualize scene index 1 (second scene in dataset)\n",
        "            2. Uses first sample (timestamp) in that scene\n",
        "            3. Calls nuScenes' render_sample() which:\n",
        "               - Loads images from all 6 cameras: CAM_FRONT, CAM_FRONT_LEFT, CAM_FRONT_RIGHT,\n",
        "                 CAM_BACK, CAM_BACK_LEFT, CAM_BACK_RIGHT\n",
        "               - Overlays 2D projections of 3D bounding boxes on each camera view\n",
        "               - Arranges in 2x3 grid (front cameras top row, back cameras bottom row)\n",
        "            4. Camera coverage:\n",
        "               - Each camera: ~70° horizontal field of view\n",
        "               - Combined: Full 360° coverage around vehicle\n",
        "            5. Saves composite image showing complete sensor suite view\n",
        "        \"\"\"\n",
        "        # Select scene index 1 (hardcoded for demo purposes)\n",
        "        my_scene = self.nusc.scene[1]\n",
        "        first_sample_token = my_scene[\"first_sample_token\"]  # Get first timestamp in scene\n",
        "\n",
        "        # Render all 6 camera views with 2D projected annotations\n",
        "        output_path = self.output_dir / f'sample_{first_sample_token}.png'\n",
        "        self.nusc.render_sample(first_sample_token)  # Creates matplotlib figure internally\n",
        "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()  # Free memory\n",
        "        print(f\"Saved sample visualization to: {output_path}\")\n",
        "\n",
        "    def visualize_sample_data(self):\n",
        "        \"\"\"\n",
        "        Render a single camera image with overlaid 2D bounding box annotations.\n",
        "\n",
        "        Visualizes one specific sensor (front camera) at a specific timestamp, showing\n",
        "        the raw camera image with 2D projections of 3D object annotations overlaid.\n",
        "\n",
        "        Technical Details:\n",
        "            1. Sensor Selection:\n",
        "               - Hardcoded to CAM_FRONT (forward-facing camera)\n",
        "               - Could be any of the 6 cameras or LIDAR_TOP sensor\n",
        "\n",
        "            2. Data Flow:\n",
        "               - sample['data'] dict maps sensor names to sample_data tokens\n",
        "               - sample_data record contains: filename, timestamp, calibration reference\n",
        "               - Retrieves CAM_FRONT's sample_data token for this timestamp\n",
        "\n",
        "            3. Rendering Process (via nuScenes API):\n",
        "               - Loads camera image from JPEG file\n",
        "               - Retrieves all 3D annotations for this sample\n",
        "               - Projects 3D boxes to 2D image plane using camera intrinsics and extrinsics\n",
        "               - Draws 2D bounding boxes on image\n",
        "               - Filters boxes by visibility and frustum culling\n",
        "\n",
        "            4. Projection Math:\n",
        "               - 3D world coords → ego vehicle frame → sensor frame → image plane\n",
        "               - Uses camera calibration matrix (intrinsics) and pose (extrinsics)\n",
        "               - Only renders objects visible in camera's field of view\n",
        "\n",
        "            5. Output: Single annotated camera image saved as PNG\n",
        "        \"\"\"\n",
        "        # Navigate to scene 1, first timestamp\n",
        "        my_scene = self.nusc.scene[1]\n",
        "        first_sample_token = my_scene[\"first_sample_token\"]\n",
        "        my_sample = self.nusc.get(\"sample\", first_sample_token)\n",
        "\n",
        "        # Select front camera sensor\n",
        "        sensor = 'CAM_FRONT'  # Could be any sensor: CAM_BACK, CAM_FRONT_LEFT, etc.\n",
        "        cam_front_data = self.nusc.get('sample_data', my_sample['data'][sensor])\n",
        "\n",
        "        # Render camera image with 2D projected annotations\n",
        "        output_path = self.output_dir / f'sample_data_{cam_front_data[\"token\"]}.png'\n",
        "        self.nusc.render_sample_data(cam_front_data['token'])  # Creates matplotlib figure\n",
        "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()  # Free memory\n",
        "        print(f\"Saved sample data visualization to: {output_path}\")\n",
        "\n",
        "    def visualize_annotation(self):\n",
        "        \"\"\"\n",
        "        Render individual object annotations across all camera views where visible.\n",
        "\n",
        "        For each annotated object, creates a visualization showing the object's 2D bounding box\n",
        "        projected onto all camera images where it appears. Demonstrates how a single 3D object\n",
        "        annotation maps to multiple 2D views.\n",
        "\n",
        "        Technical Details:\n",
        "            1. Annotation Selection:\n",
        "               - Processes first 3 annotations from scene 1's first sample\n",
        "               - Limits to 3 to avoid excessive output (samples can have 50+ annotations)\n",
        "\n",
        "            2. For Each Annotation:\n",
        "               a) Prints full annotation metadata to console:\n",
        "                  - token: Unique annotation identifier\n",
        "                  - sample_token: Parent sample (timestamp) reference\n",
        "                  - instance_token: Object instance ID (tracks same object across frames)\n",
        "                  - visibility_token: How well object is visible (1-4 scale)\n",
        "                  - category_name: Object class (e.g., 'vehicle.car', 'human.pedestrian.adult')\n",
        "                  - translation: 3D center position [x, y, z] in global coordinates (meters)\n",
        "                  - size: [width, length, height] of 3D bounding box (meters)\n",
        "                  - rotation: Quaternion [w, x, y, z] for 3D orientation\n",
        "                  - num_lidar_pts: Point cloud points inside this box\n",
        "                  - num_radar_pts: Radar detections for this object\n",
        "\n",
        "               b) Calls nuScenes' render_annotation() which:\n",
        "                  - Loads images from all 6 cameras\n",
        "                  - Projects 3D box to 2D in each camera view\n",
        "                  - Only shows cameras where object is visible (in field of view)\n",
        "                  - Creates multi-panel figure with relevant camera views\n",
        "\n",
        "            3. Use Cases:\n",
        "               - Understanding annotation structure and metadata\n",
        "               - Verifying annotation quality across multiple views\n",
        "               - Debugging object tracking and visibility\n",
        "\n",
        "            4. Output: One PNG per annotation showing all relevant camera views\n",
        "        \"\"\"\n",
        "        # Navigate to scene 1, first timestamp\n",
        "        my_scene = self.nusc.scene[1]\n",
        "        first_sample_token = my_scene[\"first_sample_token\"]\n",
        "        my_sample = self.nusc.get(\"sample\", first_sample_token)\n",
        "        annotation_tokens = my_sample['anns']  # List of all annotation tokens for this sample\n",
        "\n",
        "        # Process first 3 annotations only (for demonstration)\n",
        "        for idx, annotation_token in enumerate(annotation_tokens[:3]):\n",
        "            # Print complete annotation metadata to console for inspection\n",
        "            my_annotation_metadata = self.nusc.get('sample_annotation', annotation_token)\n",
        "            print(my_annotation_metadata)\n",
        "\n",
        "            # Render this annotation across all cameras where visible\n",
        "            self.nusc.render_annotation(annotation_token)  # Creates matplotlib figure\n",
        "            output_path = self.output_dir / f'annotation_{idx}_{annotation_token}.png'\n",
        "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close()  # Free memory\n",
        "            print(f\"Saved annotation visualization to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "PDNZ2Cl0wgPb",
      "metadata": {
        "id": "PDNZ2Cl0wgPb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DatasetConfigGenerator: Create YOLO dataset.yaml configuration file.\n",
        "\n",
        "This module generates the dataset.yaml file required by YOLO training, which\n",
        "specifies paths to train/val/test images and defines class names. The configuration\n",
        "references files directly in the preprocessed directory without duplication.\n",
        "\n",
        "Key Operations:\n",
        "- Generate dataset.yaml with absolute paths\n",
        "- Define class names and IDs\n",
        "- Save configuration for YOLO training\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "#from Globals import PREPROCESSED_ROOT, DATA_ROOT\n",
        "\n",
        "\n",
        "class DatasetConfigGenerator:\n",
        "    \"\"\"\n",
        "    Generator for YOLO dataset configuration files.\n",
        "\n",
        "    Creates a dataset.yaml file that points to the preprocessed images and labels\n",
        "    in their original locations, avoiding file duplication.\n",
        "\n",
        "    Attributes:\n",
        "        class_names: List of detection class names\n",
        "        num_classes: Number of detection classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the dataset config generator with class definitions.\n",
        "\n",
        "        Technical Details:\n",
        "            - Class IDs map to indices: 0=Car, 1=Truck/Bus, 2=Pedestrian, 3=Cyclist\n",
        "            - Order must match YOLOAnnotationConverter class_mapping\n",
        "        \"\"\"\n",
        "        self.class_names = ['car', 'truck_bus', 'pedestrian', 'cyclist']\n",
        "        self.num_classes = len(self.class_names)\n",
        "\n",
        "    def generate(self, splits, output_path):\n",
        "        \"\"\"\n",
        "        Generate YOLO dataset.yaml configuration file.\n",
        "\n",
        "        Creates a YAML file that references the preprocessed images and labels\n",
        "        directly, with separate lists for train/val/test splits.\n",
        "\n",
        "        Args:\n",
        "            splits: Dictionary from DataSplitter with train/val/test file paths\n",
        "            output_path: Path where dataset.yaml should be saved\n",
        "\n",
        "        Returns:\n",
        "            Path to the generated dataset.yaml file\n",
        "\n",
        "        Technical Details:\n",
        "\n",
        "            **YAML Structure:**\n",
        "            ```yaml\n",
        "            path: /absolute/path/to/preprocessed\n",
        "            train: images/\n",
        "            val: images/\n",
        "            test: images/\n",
        "\n",
        "            names:\n",
        "              0: car\n",
        "              1: truck_bus\n",
        "              2: pedestrian\n",
        "              3: cyclist\n",
        "\n",
        "            nc: 4\n",
        "            ```\n",
        "\n",
        "            **Path Handling:**\n",
        "            - Uses absolute paths for robustness\n",
        "            - YOLO will look for labels/ in same directory as images/\n",
        "            - Train/val/test specify subdirectories relative to path\n",
        "\n",
        "            **File Format:**\n",
        "            - Standard YAML format\n",
        "            - Compatible with Ultralytics YOLO\n",
        "            - Supports both v8 and v12 versions\n",
        "        \"\"\"\n",
        "        print(f\"\\nGenerating dataset.yaml...\")\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 1: Build dataset configuration dictionary\n",
        "        # ============================================================\n",
        "\n",
        "        # Use absolute path to preprocessed directory\n",
        "        preprocessed_path = Path(PREPROCESSED_ROOT).resolve()\n",
        "\n",
        "        # YOLO expects images and labels to be in parallel directories\n",
        "        # Since we're referencing preprocessed directory directly, we specify\n",
        "        # the images/ subdirectory for each split\n",
        "        config = {\n",
        "            'path': str(preprocessed_path),\n",
        "            'train': 'images',  # YOLO will look in path/images for train images\n",
        "            'val': 'images',    # and path/labels for train labels\n",
        "            'test': 'images',\n",
        "\n",
        "            # Class definitions\n",
        "            'names': {i: name for i, name in enumerate(self.class_names)},\n",
        "            'nc': self.num_classes\n",
        "        }\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 2: Save configuration to YAML file\n",
        "        # ============================================================\n",
        "        output_path = Path(output_path)\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "\n",
        "        print(f\"✓ Dataset configuration saved to: {output_path}\")\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 3: Create split manifest files for reference\n",
        "        # ============================================================\n",
        "        # Save train/val/test file lists for future reference\n",
        "        manifest_dir = output_path.parent / 'split_manifests'\n",
        "        manifest_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        for split_name, split_data in splits.items():\n",
        "            manifest_path = manifest_dir / f'{split_name}_files.txt'\n",
        "            with open(manifest_path, 'w') as f:\n",
        "                for img_path in split_data['images']:\n",
        "                    # Save relative path from preprocessed root\n",
        "                    rel_path = img_path.relative_to(preprocessed_path)\n",
        "                    f.write(f\"{rel_path}\\n\")\n",
        "\n",
        "            print(f\"  Saved {split_name} manifest: {manifest_path}\")\n",
        "\n",
        "        return output_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "E2r4Ui36wrY-",
      "metadata": {
        "id": "E2r4Ui36wrY-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DataSplitter: Split preprocessed dataset into train/validation/test sets.\n",
        "\n",
        "This module handles the splitting of the preprocessed BEV dataset into training,\n",
        "validation, and test subsets. The splits are performed in memory by creating lists\n",
        "of file paths - no files are copied or duplicated.\n",
        "\n",
        "Key Operations:\n",
        "- Load all preprocessed image/label pairs\n",
        "- Perform stratified train/val/test split\n",
        "- Return file path lists for each split\n",
        "- Preserve class distribution across splits\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "#from Globals import PREPROCESSED_ROOT\n",
        "\n",
        "\n",
        "class DataSplitter:\n",
        "    \"\"\"\n",
        "    Splitter for dividing preprocessed dataset into train/val/test sets in memory.\n",
        "\n",
        "    This class implements stratified sampling to ensure that the class distribution\n",
        "    is maintained across all splits. File paths are organized into lists without\n",
        "    copying or moving any actual files.\n",
        "\n",
        "    Attributes:\n",
        "        preprocessed_root: Path to preprocessed dataset directory\n",
        "        images_dir: Path to preprocessed images directory\n",
        "        labels_dir: Path to preprocessed labels directory\n",
        "        train_ratio: Fraction of data for training (default: 0.7)\n",
        "        val_ratio: Fraction of data for validation (default: 0.15)\n",
        "        test_ratio: Fraction of data for testing (default: 0.15)\n",
        "        random_seed: Random seed for reproducibility\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_seed=42):\n",
        "        \"\"\"\n",
        "        Initialize the data splitter with split ratios.\n",
        "\n",
        "        Args:\n",
        "            train_ratio: Fraction of data for training (0 < x < 1)\n",
        "            val_ratio: Fraction of data for validation (0 < x < 1)\n",
        "            test_ratio: Fraction of data for testing (0 < x < 1)\n",
        "            random_seed: Random seed for reproducible splits\n",
        "\n",
        "        Technical Details:\n",
        "            - Ratios must sum to 1.0\n",
        "            - Typical splits: 70/15/15 or 80/10/10\n",
        "            - Random seed ensures same split across runs\n",
        "        \"\"\"\n",
        "        # Validate ratios sum to 1.0\n",
        "        total = train_ratio + val_ratio + test_ratio\n",
        "        if abs(total - 1.0) > 1e-6:\n",
        "            raise ValueError(f\"Split ratios must sum to 1.0, got {total}\")\n",
        "\n",
        "        self.train_ratio = train_ratio\n",
        "        self.val_ratio = val_ratio\n",
        "        self.test_ratio = test_ratio\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "        # Set random seeds for reproducibility\n",
        "        random.seed(random_seed)\n",
        "\n",
        "        # Setup paths\n",
        "        self.preprocessed_root = Path(PREPROCESSED_ROOT)\n",
        "        self.images_dir = self.preprocessed_root / 'images'\n",
        "        self.labels_dir = self.preprocessed_root / 'labels'\n",
        "\n",
        "    def split(self):\n",
        "        \"\"\"\n",
        "        Split preprocessed dataset into train/val/test sets in memory.\n",
        "\n",
        "        Returns file path lists without copying any files. The returned paths\n",
        "        reference the original files in the preprocessed directory.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with keys 'train', 'val', 'test', each containing:\n",
        "            {\n",
        "                'images': List of Path objects to image files,\n",
        "                'labels': List of Path objects to label files\n",
        "            }\n",
        "\n",
        "        Technical Details:\n",
        "            - Two-stage split: first separate test, then split remainder into train/val\n",
        "            - File paths are kept as Path objects, no files copied\n",
        "            - Shuffle ensures random distribution\n",
        "            - Random seed provides reproducibility\n",
        "        \"\"\"\n",
        "        # ============================================================\n",
        "        # STEP 1: Get all image files\n",
        "        # ============================================================\n",
        "        print(\"Scanning preprocessed dataset...\")\n",
        "        image_files = sorted(list(self.images_dir.glob('*.png')))\n",
        "\n",
        "        if len(image_files) == 0:\n",
        "            raise FileNotFoundError(\n",
        "                f\"No preprocessed images found in {self.images_dir}\\n\"\n",
        "                f\"Have you run the preprocessing stage?\"\n",
        "            )\n",
        "\n",
        "        print(f\"Found {len(image_files)} preprocessed samples\")\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 2: Create list of filename stems\n",
        "        # ============================================================\n",
        "        file_stems = [f.stem for f in image_files]\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 3: First split - separate test set\n",
        "        # ============================================================\n",
        "        train_val_stems, test_stems = train_test_split(\n",
        "            file_stems,\n",
        "            test_size=self.test_ratio,\n",
        "            random_state=self.random_seed,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 4: Second split - separate train and val\n",
        "        # ============================================================\n",
        "        val_ratio_adjusted = self.val_ratio / (self.train_ratio + self.val_ratio)\n",
        "\n",
        "        train_stems, val_stems = train_test_split(\n",
        "            train_val_stems,\n",
        "            test_size=val_ratio_adjusted,\n",
        "            random_state=self.random_seed,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 5: Build file path dictionaries (no copying)\n",
        "        # ============================================================\n",
        "        splits = {\n",
        "            'train': self._build_file_lists(train_stems),\n",
        "            'val': self._build_file_lists(val_stems),\n",
        "            'test': self._build_file_lists(test_stems)\n",
        "        }\n",
        "\n",
        "        # Print split summary\n",
        "        print(f\"\\nDataset split:\")\n",
        "        print(f\"  Train: {len(train_stems)} samples ({len(train_stems)/len(file_stems)*100:.1f}%)\")\n",
        "        print(f\"  Val:   {len(val_stems)} samples ({len(val_stems)/len(file_stems)*100:.1f}%)\")\n",
        "        print(f\"  Test:  {len(test_stems)} samples ({len(test_stems)/len(file_stems)*100:.1f}%)\")\n",
        "\n",
        "        return splits\n",
        "\n",
        "    def _build_file_lists(self, file_stems):\n",
        "        \"\"\"\n",
        "        Build lists of image and label file paths from filename stems.\n",
        "\n",
        "        Args:\n",
        "            file_stems: List of filename stems (without extensions)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with 'images' and 'labels' keys containing Path lists\n",
        "        \"\"\"\n",
        "        images = [self.images_dir / f\"{stem}.png\" for stem in file_stems]\n",
        "        labels = [self.labels_dir / f\"{stem}.txt\" for stem in file_stems]\n",
        "\n",
        "        return {\n",
        "            'images': images,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cK86sGGtwwFX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK86sGGtwwFX",
        "outputId": "81ab0219-9775-4307-bbd5-14986b647dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "ModelEvaluator: Evaluate trained YOLO model on test set.\n",
        "\n",
        "This module handles model evaluation including inference on test set, metrics\n",
        "computation (mAP, precision, recall), and performance analysis. It provides\n",
        "comprehensive assessment of detection performance.\n",
        "\n",
        "Key Operations:\n",
        "- Run inference on test set\n",
        "- Compute detection metrics (mAP@0.5, mAP@0.5:0.95)\n",
        "- Calculate per-class performance\n",
        "- Measure inference speed\n",
        "\"\"\"\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluator for trained YOLO detection models.\n",
        "\n",
        "    This class handles model evaluation on test data, computing standard object\n",
        "    detection metrics and analyzing performance across different classes and\n",
        "    confidence thresholds.\n",
        "\n",
        "    Attributes:\n",
        "        model: YOLO model instance loaded from weights\n",
        "        model_path: Path to model weights file\n",
        "        dataset_yaml: Path to dataset configuration\n",
        "        class_names: List of detection class names\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path, dataset_yaml):\n",
        "        \"\"\"\n",
        "        Initialize the model evaluator.\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to trained model weights (.pt file)\n",
        "            dataset_yaml: Path to dataset.yaml configuration\n",
        "\n",
        "        Technical Details:\n",
        "            - Loads model from weights file\n",
        "            - Validates model architecture\n",
        "            - Prepares for evaluation on test set\n",
        "        \"\"\"\n",
        "        self.model_path = Path(model_path)\n",
        "        self.dataset_yaml = str(dataset_yaml)\n",
        "        self.class_names = ['Car', 'Truck/Bus', 'Pedestrian', 'Cyclist']\n",
        "\n",
        "        # Load trained model\n",
        "        print(f\"\\nLoading model from: {self.model_path}\")\n",
        "        if not self.model_path.exists():\n",
        "            raise FileNotFoundError(f\"Model weights not found: {self.model_path}\")\n",
        "\n",
        "        self.model = YOLO(str(self.model_path))\n",
        "        print(\"✓ Model loaded successfully\")\n",
        "\n",
        "    def evaluate(self, conf_threshold=0.25, iou_threshold=0.45, img_size=1000):\n",
        "        \"\"\"\n",
        "        Evaluate model on test set.\n",
        "\n",
        "        Runs inference on all test images and computes comprehensive detection\n",
        "        metrics including mAP, precision, recall, and per-class performance.\n",
        "\n",
        "        Args:\n",
        "            conf_threshold: Confidence threshold for predictions (default: 0.25)\n",
        "            iou_threshold: IoU threshold for NMS (default: 0.45)\n",
        "            img_size: Input image size (default: 1000)\n",
        "\n",
        "        Returns:\n",
        "            Results object containing metrics and predictions\n",
        "\n",
        "        Technical Details:\n",
        "\n",
        "            **Metrics Computed:**\n",
        "\n",
        "            • mAP@0.5 (Mean Average Precision at IoU=0.5):\n",
        "              - Standard COCO metric for loose localization\n",
        "              - Considers detection \"correct\" if IoU ≥ 0.5\n",
        "              - Values typically 0.5-0.9 for good detectors\n",
        "\n",
        "            • mAP@0.5:0.95 (Mean Average Precision at IoU=0.5:0.95):\n",
        "              - Averaged over IoU thresholds from 0.5 to 0.95 (step 0.05)\n",
        "              - More stringent metric requiring tighter localization\n",
        "              - Values typically 0.3-0.6 for good detectors\n",
        "\n",
        "            • Precision:\n",
        "              - TP / (TP + FP)\n",
        "              - Proportion of correct detections among all detections\n",
        "              - Higher = fewer false alarms\n",
        "\n",
        "            • Recall:\n",
        "              - TP / (TP + FN)\n",
        "              - Proportion of ground truth objects detected\n",
        "              - Higher = fewer missed objects\n",
        "\n",
        "            **Confidence Threshold:**\n",
        "            - 0.25: Default, balances precision and recall\n",
        "            - Lower: More detections, higher recall, lower precision\n",
        "            - Higher: Fewer detections, lower recall, higher precision\n",
        "\n",
        "            **NMS IoU Threshold:**\n",
        "            - 0.45: Removes overlapping boxes (keep best)\n",
        "            - Lower: More aggressive suppression\n",
        "            - Higher: Keeps more overlapping detections\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"MODEL EVALUATION\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nEvaluation settings:\")\n",
        "        print(f\"  Confidence threshold: {conf_threshold}\")\n",
        "        print(f\"  NMS IoU threshold: {iou_threshold}\")\n",
        "        print(f\"  Image size: {img_size}\")\n",
        "\n",
        "        # ============================================================\n",
        "        # Run validation on test split\n",
        "        # ============================================================\n",
        "        print(f\"\\nRunning inference on test set...\")\n",
        "\n",
        "        results = self.model.val(\n",
        "            data=self.dataset_yaml,\n",
        "            split='test',\n",
        "            imgsz=img_size,\n",
        "            batch=16,\n",
        "            conf=conf_threshold,\n",
        "            iou=iou_threshold,\n",
        "            plots=True,\n",
        "            save_json=True,\n",
        "            save_txt=True\n",
        "        )\n",
        "\n",
        "        print(\"✓ Evaluation complete\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_metrics(self, results):\n",
        "        \"\"\"\n",
        "        Print comprehensive evaluation metrics.\n",
        "\n",
        "        Displays overall and per-class performance metrics in a formatted\n",
        "        table for easy interpretation.\n",
        "\n",
        "        Args:\n",
        "            results: Results object from evaluate()\n",
        "\n",
        "        Technical Details:\n",
        "            - Extracts metrics from YOLO results object\n",
        "            - Formats for console display\n",
        "            - Includes inference timing statistics\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EVALUATION METRICS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # ============================================================\n",
        "        # Overall metrics\n",
        "        # ============================================================\n",
        "        print(f\"\\nOverall Performance:\")\n",
        "        print(f\"  mAP@0.5:      {results.box.map50:.4f}\")\n",
        "        print(f\"  mAP@0.5:0.95: {results.box.map:.4f}\")\n",
        "        print(f\"  Precision:    {results.box.mp:.4f}\")\n",
        "        print(f\"  Recall:       {results.box.mr:.4f}\")\n",
        "\n",
        "        # ============================================================\n",
        "        # Per-class metrics\n",
        "        # ============================================================\n",
        "        print(f\"\\nPer-Class Performance:\")\n",
        "        print(f\"  {'Class':<15} {'mAP@0.5':<10} {'Precision':<12} {'Recall':<10}\")\n",
        "        print(f\"  {'-'*50}\")\n",
        "\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            map50 = results.box.maps[i] if hasattr(results.box, 'maps') else 0.0\n",
        "            precision = results.box.p[i] if hasattr(results.box, 'p') else 0.0\n",
        "            recall = results.box.r[i] if hasattr(results.box, 'r') else 0.0\n",
        "\n",
        "            print(f\"  {class_name:<15} {map50:<10.4f} {precision:<12.4f} {recall:<10.4f}\")\n",
        "\n",
        "        # ============================================================\n",
        "        # Inference speed\n",
        "        # ============================================================\n",
        "        print(f\"\\nInference Speed:\")\n",
        "        if hasattr(results, 'speed'):\n",
        "            preprocess_time = results.speed.get('preprocess', 0)\n",
        "            inference_time = results.speed.get('inference', 0)\n",
        "            postprocess_time = results.speed.get('postprocess', 0)\n",
        "            total_time = preprocess_time + inference_time + postprocess_time\n",
        "\n",
        "            print(f\"  Preprocess:  {preprocess_time:.2f} ms\")\n",
        "            print(f\"  Inference:   {inference_time:.2f} ms\")\n",
        "            print(f\"  Postprocess: {postprocess_time:.2f} ms\")\n",
        "            print(f\"  Total:       {total_time:.2f} ms\")\n",
        "            print(f\"  FPS:         {1000 / total_time:.2f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    def predict_batch(self, image_paths, conf_threshold=0.25, save_dir=None):\n",
        "        \"\"\"\n",
        "        Run inference on a batch of images.\n",
        "\n",
        "        Useful for visualizing predictions on specific images or creating\n",
        "        demo outputs.\n",
        "\n",
        "        Args:\n",
        "            image_paths: List of paths to images\n",
        "            conf_threshold: Confidence threshold (default: 0.25)\n",
        "            save_dir: Directory to save prediction visualizations (optional)\n",
        "\n",
        "        Returns:\n",
        "            List of prediction results, one per image\n",
        "\n",
        "        Technical Details:\n",
        "            - Processes images in batch for efficiency\n",
        "            - Optionally saves annotated images\n",
        "            - Returns raw prediction results for further processing\n",
        "        \"\"\"\n",
        "        results = self.model.predict(\n",
        "            source=image_paths,\n",
        "            conf=conf_threshold,\n",
        "            save=save_dir is not None,\n",
        "            project=save_dir,\n",
        "            exist_ok=True\n",
        "        )\n",
        "\n",
        "        return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "XqLuQvk6N6Ml",
      "metadata": {
        "id": "XqLuQvk6N6Ml"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PerformanceAnalyzer: Analyze model performance across different scenarios.\n",
        "\n",
        "This module provides detailed performance analysis including per-class metrics,\n",
        "confidence threshold analysis, and error pattern identification.\n",
        "\n",
        "Key Operations:\n",
        "- Compute detailed per-class statistics\n",
        "- Analyze performance across confidence thresholds\n",
        "- Identify common failure modes\n",
        "- Generate performance summary tables\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class PerformanceAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyzer for detailed model performance assessment.\n",
        "\n",
        "    Provides tools for understanding model behavior beyond basic metrics,\n",
        "    including analysis of failure cases and performance across different\n",
        "    object types and scenarios.\n",
        "\n",
        "    Attributes:\n",
        "        class_names: List of detection class names\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the performance analyzer.\n",
        "\n",
        "        Technical Details:\n",
        "            - Prepares analysis frameworks\n",
        "            - Sets up metric tracking structures\n",
        "        \"\"\"\n",
        "        self.class_names = ['Car', 'Truck/Bus', 'Pedestrian', 'Cyclist']\n",
        "\n",
        "    def analyze_class_distribution(self, labels_dir):\n",
        "        \"\"\"\n",
        "        Analyze class distribution in the dataset.\n",
        "\n",
        "        Counts instances of each class across all labels to understand\n",
        "        dataset balance and potential class imbalance issues.\n",
        "\n",
        "        Args:\n",
        "            labels_dir: Directory containing YOLO label files\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping class names to instance counts\n",
        "\n",
        "        Technical Details:\n",
        "            - Parses all label files\n",
        "            - Aggregates class counts\n",
        "            - Identifies imbalanced classes\n",
        "        \"\"\"\n",
        "        print(\"\\nAnalyzing class distribution...\")\n",
        "\n",
        "        labels_dir = Path(labels_dir)\n",
        "        class_counts = {name: 0 for name in self.class_names}\n",
        "\n",
        "        # Count instances in each label file\n",
        "        for label_file in labels_dir.glob('*.txt'):\n",
        "            with open(label_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) == 5:\n",
        "                        class_id = int(parts[0])\n",
        "                        if 0 <= class_id < len(self.class_names):\n",
        "                            class_counts[self.class_names[class_id]] += 1\n",
        "\n",
        "        # Print distribution\n",
        "        total = sum(class_counts.values())\n",
        "        print(\"\\nClass Distribution:\")\n",
        "        print(f\"  {'Class':<15} {'Count':<10} {'Percentage':<10}\")\n",
        "        print(f\"  {'-'*40}\")\n",
        "\n",
        "        for class_name, count in class_counts.items():\n",
        "            percentage = (count / total * 100) if total > 0 else 0\n",
        "            print(f\"  {class_name:<15} {count:<10} {percentage:>6.2f}%\")\n",
        "\n",
        "        print(f\"\\n  Total: {total}\")\n",
        "\n",
        "        return class_counts\n",
        "\n",
        "    def compute_performance_summary(self, results):\n",
        "        \"\"\"\n",
        "        Compute comprehensive performance summary.\n",
        "\n",
        "        Extracts and organizes all key metrics from evaluation results\n",
        "        into a structured summary for reporting or further analysis.\n",
        "\n",
        "        Args:\n",
        "            results: Evaluation results from ModelEvaluator\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing organized performance metrics\n",
        "\n",
        "        Technical Details:\n",
        "            - Extracts metrics from YOLO results object\n",
        "            - Organizes by category (overall, per-class, speed)\n",
        "            - Returns structured data for downstream use\n",
        "        \"\"\"\n",
        "        summary = {\n",
        "            'overall': {\n",
        "                'mAP_50': float(results.box.map50),\n",
        "                'mAP_50_95': float(results.box.map),\n",
        "                'precision': float(results.box.mp),\n",
        "                'recall': float(results.box.mr)\n",
        "            },\n",
        "            'per_class': {},\n",
        "            'speed': {}\n",
        "        }\n",
        "\n",
        "        # Per-class metrics\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            summary['per_class'][class_name] = {\n",
        "                'mAP_50': float(results.box.maps[i]) if hasattr(results.box, 'maps') else 0.0,\n",
        "                'precision': float(results.box.p[i]) if hasattr(results.box, 'p') else 0.0,\n",
        "                'recall': float(results.box.r[i]) if hasattr(results.box, 'r') else 0.0\n",
        "            }\n",
        "\n",
        "        # Speed metrics\n",
        "        if hasattr(results, 'speed'):\n",
        "            summary['speed'] = {\n",
        "                'preprocess_ms': results.speed.get('preprocess', 0),\n",
        "                'inference_ms': results.speed.get('inference', 0),\n",
        "                'postprocess_ms': results.speed.get('postprocess', 0),\n",
        "                'total_ms': sum(results.speed.values()),\n",
        "                'fps': 1000 / sum(results.speed.values()) if sum(results.speed.values()) > 0 else 0\n",
        "            }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def print_summary(self, summary):\n",
        "        \"\"\"\n",
        "        Print formatted performance summary.\n",
        "\n",
        "        Args:\n",
        "            summary: Performance summary dictionary from compute_performance_summary()\n",
        "\n",
        "        Technical Details:\n",
        "            - Pretty-prints structured metrics\n",
        "            - Formatted for console display\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PERFORMANCE SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\nOverall Metrics:\")\n",
        "        for metric, value in summary['overall'].items():\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\nPer-Class Performance:\")\n",
        "        for class_name, metrics in summary['per_class'].items():\n",
        "            print(f\"\\n  {class_name}:\")\n",
        "            for metric, value in metrics.items():\n",
        "                print(f\"    {metric}: {value:.4f}\")\n",
        "\n",
        "        if summary['speed']:\n",
        "            print(\"\\nInference Speed:\")\n",
        "            for metric, value in summary['speed'].items():\n",
        "                if 'fps' in metric:\n",
        "                    print(f\"  {metric}: {value:.2f}\")\n",
        "                else:\n",
        "                    print(f\"  {metric}: {value:.2f} ms\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "mgE9ZLSEOPrz",
      "metadata": {
        "id": "mgE9ZLSEOPrz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ResultsVisualizer: Visualize model predictions and performance.\n",
        "\n",
        "This module creates visualizations of model predictions including detection\n",
        "boxes overlaid on BEV images, confusion matrices, and performance plots.\n",
        "\n",
        "Key Operations:\n",
        "- Visualize predictions on test images\n",
        "- Generate confusion matrix\n",
        "- Plot precision-recall curves\n",
        "- Create performance comparison charts\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import random\n",
        "#from Globals import RESULTS_ROOT\n",
        "\n",
        "\n",
        "class ResultsVisualizer:\n",
        "    \"\"\"\n",
        "    Visualizer for model predictions and evaluation results.\n",
        "\n",
        "    Creates visual outputs to assess model performance and understand prediction\n",
        "    behavior across different scenarios.\n",
        "\n",
        "    Attributes:\n",
        "        results_dir: Directory for saving visualization outputs\n",
        "        class_names: List of detection class names\n",
        "        colors: Color scheme for each class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the results visualizer.\n",
        "\n",
        "        Technical Details:\n",
        "            - Creates results directory structure\n",
        "            - Sets up color scheme for consistent visualization\n",
        "            - Configures matplotlib defaults\n",
        "        \"\"\"\n",
        "        self.results_dir = Path(RESULTS_ROOT)\n",
        "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.class_names = ['Car', 'Truck/Bus', 'Pedestrian', 'Cyclist']\n",
        "        self.colors = [\n",
        "            (255, 0, 0),    # Red: Cars\n",
        "            (0, 255, 0),    # Green: Trucks/Buses\n",
        "            (0, 0, 255),    # Blue: Pedestrians\n",
        "            (255, 255, 0)   # Yellow: Cyclists\n",
        "        ]\n",
        "\n",
        "    def visualize_predictions(self, model, test_images_dir, num_samples=10, conf_threshold=0.25):\n",
        "        \"\"\"\n",
        "        Visualize model predictions on random test images.\n",
        "\n",
        "        Selects random images from test set, runs inference, and creates\n",
        "        visualizations with predicted bounding boxes overlaid.\n",
        "\n",
        "        Args:\n",
        "            model: Trained YOLO model instance\n",
        "            test_images_dir: Directory containing test images\n",
        "            num_samples: Number of images to visualize (default: 10)\n",
        "            conf_threshold: Confidence threshold for predictions (default: 0.25)\n",
        "\n",
        "        Returns:\n",
        "            None (saves visualizations to disk)\n",
        "\n",
        "        Technical Details:\n",
        "            - Random sampling ensures diverse visualization\n",
        "            - Predictions drawn with confidence scores\n",
        "            - Color-coded by class\n",
        "            - Saved as high-resolution PNG files\n",
        "        \"\"\"\n",
        "        print(f\"\\nGenerating prediction visualizations...\")\n",
        "\n",
        "        # ============================================================\n",
        "        # Select random test images\n",
        "        # ============================================================\n",
        "        test_images_dir = Path(test_images_dir)\n",
        "        all_images = list(test_images_dir.glob('*.png'))\n",
        "\n",
        "        if len(all_images) == 0:\n",
        "            print(f\"Warning: No test images found in {test_images_dir}\")\n",
        "            return\n",
        "\n",
        "        sample_images = random.sample(all_images, min(num_samples, len(all_images)))\n",
        "\n",
        "        # ============================================================\n",
        "        # Create output directory\n",
        "        # ============================================================\n",
        "        vis_dir = self.results_dir / 'predictions'\n",
        "        vis_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # ============================================================\n",
        "        # Run predictions and visualize\n",
        "        # ============================================================\n",
        "        for img_path in sample_images:\n",
        "            # Run inference\n",
        "            results = model.predict(\n",
        "                source=str(img_path),\n",
        "                conf=conf_threshold,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Get first result (single image)\n",
        "            result = results[0]\n",
        "\n",
        "            # Load original image\n",
        "            img = cv2.imread(str(img_path))\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Draw predictions\n",
        "            if result.boxes is not None and len(result.boxes) > 0:\n",
        "                boxes = result.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2\n",
        "                confs = result.boxes.conf.cpu().numpy()  # Confidence scores\n",
        "                classes = result.boxes.cls.cpu().numpy().astype(int)  # Class IDs\n",
        "\n",
        "                for box, conf, cls in zip(boxes, confs, classes):\n",
        "                    x1, y1, x2, y2 = box.astype(int)\n",
        "\n",
        "                    # Draw bounding box\n",
        "                    cv2.rectangle(img_rgb, (x1, y1), (x2, y2), self.colors[cls], 2)\n",
        "\n",
        "                    # Draw label with confidence\n",
        "                    label = f\"{self.class_names[cls]}: {conf:.2f}\"\n",
        "                    cv2.putText(img_rgb, label, (x1, y1-10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors[cls], 2)\n",
        "\n",
        "            # Save visualization\n",
        "            output_path = vis_dir / f\"pred_{img_path.name}\"\n",
        "            plt.figure(figsize=(15, 15))\n",
        "            plt.imshow(img_rgb)\n",
        "            plt.title(f\"Predictions: {img_path.name}\\nDetections: {len(result.boxes) if result.boxes else 0}\")\n",
        "            plt.axis('off')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        print(f\"✓ Saved {len(sample_images)} prediction visualizations to: {vis_dir}\")\n",
        "\n",
        "    def generate_performance_report(self, results, output_path=None):\n",
        "        \"\"\"\n",
        "        Generate comprehensive performance report.\n",
        "\n",
        "        Creates a text report summarizing model performance including overall\n",
        "        metrics, per-class breakdown, and inference timing.\n",
        "\n",
        "        Args:\n",
        "            results: Evaluation results from ModelEvaluator\n",
        "            output_path: Path for report file (default: results/evaluation_report.txt)\n",
        "\n",
        "        Returns:\n",
        "            Path to generated report file\n",
        "\n",
        "        Technical Details:\n",
        "            - Plain text format for easy viewing\n",
        "            - Includes all key metrics\n",
        "            - Suitable for documentation or sharing\n",
        "        \"\"\"\n",
        "        if output_path is None:\n",
        "            output_path = self.results_dir / 'evaluation_report.txt'\n",
        "        else:\n",
        "            output_path = Path(output_path)\n",
        "\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(f\"\\nGenerating performance report...\")\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "            f.write(\"nuScenes BEV Object Detection - Evaluation Report\\n\")\n",
        "            f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"Model Configuration:\\n\")\n",
        "            f.write(\"-\"*80 + \"\\n\")\n",
        "            f.write(\"Base Model: YOLOv12s\\n\")\n",
        "            f.write(\"Training Strategy: Two-stage transfer learning\\n\")\n",
        "            f.write(\"Input Resolution: 1000×1000 pixels\\n\")\n",
        "            f.write(\"Detection Classes: 4 (Car, Truck/Bus, Pedestrian, Cyclist)\\n\\n\")\n",
        "\n",
        "            f.write(\"Overall Performance:\\n\")\n",
        "            f.write(\"-\"*80 + \"\\n\")\n",
        "            f.write(f\"mAP@0.5:      {results.box.map50:.4f}\\n\")\n",
        "            f.write(f\"mAP@0.5:0.95: {results.box.map:.4f}\\n\")\n",
        "            f.write(f\"Precision:    {results.box.mp:.4f}\\n\")\n",
        "            f.write(f\"Recall:       {results.box.mr:.4f}\\n\\n\")\n",
        "\n",
        "            f.write(\"Per-Class Performance:\\n\")\n",
        "            f.write(\"-\"*80 + \"\\n\")\n",
        "            f.write(f\"{'Class':<15} {'mAP@0.5':<12} {'Precision':<12} {'Recall':<10}\\n\")\n",
        "            f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "            for i, class_name in enumerate(self.class_names):\n",
        "                map50 = results.box.maps[i] if hasattr(results.box, 'maps') else 0.0\n",
        "                precision = results.box.p[i] if hasattr(results.box, 'p') else 0.0\n",
        "                recall = results.box.r[i] if hasattr(results.box, 'r') else 0.0\n",
        "\n",
        "                f.write(f\"{class_name:<15} {map50:<12.4f} {precision:<12.4f} {recall:<10.4f}\\n\")\n",
        "\n",
        "            if hasattr(results, 'speed'):\n",
        "                total_time = sum(results.speed.values())\n",
        "                fps = 1000 / total_time if total_time > 0 else 0\n",
        "\n",
        "                f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "                f.write(\"Inference Speed:\\n\")\n",
        "                f.write(\"-\"*80 + \"\\n\")\n",
        "                f.write(f\"Average FPS: {fps:.2f}\\n\")\n",
        "                f.write(f\"Total latency: {total_time:.2f} ms\\n\\n\")\n",
        "\n",
        "            f.write(\"=\"*80 + \"\\n\")\n",
        "            f.write(\"Conclusion:\\n\")\n",
        "            f.write(\"-\"*80 + \"\\n\")\n",
        "            f.write(\"The model demonstrates the feasibility of using YOLO for LiDAR-based\\n\")\n",
        "            f.write(\"object detection by converting 3D point clouds to BEV representations.\\n\")\n",
        "            f.write(\"The two-stage training approach with transfer learning enables efficient\\n\")\n",
        "            f.write(\"domain adaptation from COCO to the nuScenes BEV dataset.\\n\")\n",
        "\n",
        "        print(f\"✓ Report saved to: {output_path}\")\n",
        "        return output_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "vpeQ-5dxOWGC",
      "metadata": {
        "id": "vpeQ-5dxOWGC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "BEVInspector: Visualize and validate preprocessed BEV dataset with YOLO annotations.\n",
        "\n",
        "This module provides tools for inspecting the preprocessed BEV dataset by loading\n",
        "BEV images and their corresponding YOLO annotations, then visualizing the bounding\n",
        "boxes overlaid on the images. This is essential for:\n",
        "- Validating preprocessing pipeline correctness\n",
        "- Debugging annotation alignment issues\n",
        "- Visual quality assessment of the dataset\n",
        "- Understanding class distribution and box sizes\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import os\n",
        "#from Globals import PREPROCESSED_ROOT\n",
        "\n",
        "\n",
        "class BEVInspector:\n",
        "    \"\"\"\n",
        "    Inspector for visualizing preprocessed BEV images with YOLO bounding boxes.\n",
        "\n",
        "    This class loads pairs of BEV images and their corresponding YOLO label files,\n",
        "    renders the bounding boxes on the images with color-coded class labels, and\n",
        "    displays them for visual inspection and quality control.\n",
        "\n",
        "    Visualization helps identify issues such as:\n",
        "    - Misaligned bounding boxes (coordinate transformation errors)\n",
        "    - Missing or extra annotations\n",
        "    - Incorrect class labels\n",
        "    - Box size anomalies\n",
        "\n",
        "    Attributes:\n",
        "        class_names: List of human-readable class names [Car, Truck/Bus, Pedestrian, Cyclist]\n",
        "        colors: List of BGR color tuples for each class (for OpenCV rendering)\n",
        "        images_dir: Path to directory containing BEV images\n",
        "        labels_dir: Path to directory containing YOLO label files\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the BEV inspector with class definitions and data paths.\n",
        "\n",
        "        Technical Details:\n",
        "            - Class names map to class IDs [0, 1, 2, 3]\n",
        "            - Colors are in BGR format (OpenCV convention, not RGB)\n",
        "            - Color scheme:\n",
        "              * Red (255,0,0): Cars - most common, high visibility\n",
        "              * Green (0,255,0): Trucks/Buses - large vehicles\n",
        "              * Blue (0,0,255): Pedestrians - vulnerable road users\n",
        "              * Yellow (255,255,0): Cyclists - two-wheeled vehicles\n",
        "        \"\"\"\n",
        "        # Human-readable class labels (index = class ID)\n",
        "        self.class_names = ['Car', 'Truck/Bus', 'Pedestrian', 'Cyclist']\n",
        "\n",
        "        # Colors for bounding boxes (BGR format for OpenCV)\n",
        "        self.colors = [\n",
        "            (255, 0, 0),   # Class 0 (Car): Red\n",
        "            (0, 255, 0),   # Class 1 (Truck/Bus): Green\n",
        "            (0, 0, 255),   # Class 2 (Pedestrian): Blue\n",
        "            (255, 255, 0)  # Class 3 (Cyclist): Yellow\n",
        "        ]\n",
        "\n",
        "        # Paths to preprocessed dataset directories\n",
        "        self.images_dir = Path(PREPROCESSED_ROOT) / 'images'\n",
        "        self.labels_dir = Path(PREPROCESSED_ROOT) / 'labels'\n",
        "\n",
        "    def load_samples(self, num_samples):\n",
        "        \"\"\"\n",
        "        Load a subset of BEV images and their corresponding YOLO labels.\n",
        "\n",
        "        This method samples images uniformly across the dataset to provide a\n",
        "        representative view of the preprocessed data without loading everything.\n",
        "\n",
        "        Args:\n",
        "            num_samples: Number of image-label pairs to load\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (bev_images, yolo_labels_list) where:\n",
        "            - bev_images: List of numpy arrays (BGR images from OpenCV)\n",
        "            - yolo_labels_list: List of lists, each containing YOLO annotations\n",
        "              Format per annotation: [class_id, x_center, y_center, width, height]\n",
        "\n",
        "        Technical Details:\n",
        "            1. Sampling Strategy:\n",
        "               - Lists all PNG files in images directory\n",
        "               - Calculates uniform step size to select evenly distributed samples\n",
        "               - Avoids random sampling to ensure reproducibility\n",
        "\n",
        "            2. File Loading:\n",
        "               - Images: cv2.imread loads as BGR uint8 arrays\n",
        "               - Labels: Text file parsed line-by-line\n",
        "               - Matching: Uses image filename stem to find corresponding label\n",
        "\n",
        "            3. Label Parsing:\n",
        "               - Splits each line by whitespace\n",
        "               - Expects exactly 5 values per line\n",
        "               - First value (class_id) converted to int\n",
        "               - Remaining values (x, y, w, h) converted to float\n",
        "               - Invalid lines (wrong format) are skipped\n",
        "\n",
        "            4. Error Handling:\n",
        "               - Missing label files result in empty label list (not an error)\n",
        "               - Malformed lines within label files are silently skipped\n",
        "               - This gracefully handles incomplete preprocessing\n",
        "        \"\"\"\n",
        "        # Get all BEV image files, sorted for consistent ordering\n",
        "        image_files = sorted(list(self.images_dir.glob('*.png')))\n",
        "\n",
        "        # Calculate step size for uniform sampling across dataset\n",
        "        step = max(1, len(image_files) // num_samples)\n",
        "\n",
        "        # Select evenly-spaced images (list comprehension with stride)\n",
        "        selected_files = [image_files[i * step] for i in range(num_samples) if i * step < len(image_files)]\n",
        "\n",
        "        # Initialize lists to collect loaded data\n",
        "        bev_images = []\n",
        "        yolo_labels_list = []\n",
        "\n",
        "        # Load each selected image and its corresponding labels\n",
        "        for image_path in selected_files:\n",
        "            # Load BEV image (BGR format, uint8)\n",
        "            bev_image = cv2.imread(str(image_path))\n",
        "\n",
        "            # Find matching label file (same filename, different extension)\n",
        "            label_path = self.labels_dir / f\"{image_path.stem}.txt\"\n",
        "            yolo_labels = []\n",
        "\n",
        "            # Parse YOLO labels if file exists\n",
        "            if label_path.exists():\n",
        "                with open(label_path, 'r') as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()  # Split by whitespace\n",
        "\n",
        "                        # Validate format: must have exactly 5 values\n",
        "                        if len(parts) == 5:\n",
        "                            # Parse: <class_id> <x> <y> <w> <h>\n",
        "                            yolo_labels.append(\n",
        "                                [int(parts[0])] +  # class_id as integer\n",
        "                                [float(x) for x in parts[1:]]  # coordinates as floats\n",
        "                            )\n",
        "\n",
        "            # Add to collections\n",
        "            bev_images.append(bev_image)\n",
        "            yolo_labels_list.append(yolo_labels)\n",
        "\n",
        "        return bev_images, yolo_labels_list\n",
        "\n",
        "    def visualize(self, bev_image, yolo_labels):\n",
        "        \"\"\"\n",
        "        Visualize a single BEV image with bounding boxes and labels overlaid.\n",
        "\n",
        "        Creates a matplotlib figure showing the BEV image with color-coded bounding\n",
        "        boxes drawn for each object, along with class name labels.\n",
        "\n",
        "        Args:\n",
        "            bev_image: numpy array (H, W, 3) in BGR format from OpenCV\n",
        "            yolo_labels: List of annotations, each [class_id, x, y, w, h] normalized\n",
        "\n",
        "        Technical Details:\n",
        "            1. Color Space Conversion:\n",
        "               - Input: BGR (OpenCV format)\n",
        "               - Output: RGB (matplotlib format)\n",
        "               - Required because OpenCV and matplotlib use different conventions\n",
        "\n",
        "            2. Coordinate Denormalization:\n",
        "               - YOLO format uses normalized coordinates [0, 1]\n",
        "               - Must multiply by image dimensions to get pixel coordinates\n",
        "               - x_pixel = x_normalized * image_width\n",
        "\n",
        "            3. Box Format Conversion:\n",
        "               - YOLO: (center_x, center_y, width, height)\n",
        "               - OpenCV rectangle: (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
        "               - Conversion: top_left = center - size/2, bottom_right = center + size/2\n",
        "\n",
        "            4. Rendering:\n",
        "               - Boxes drawn with 2-pixel thickness\n",
        "               - Class names positioned above top-left corner\n",
        "               - Colors match class definitions\n",
        "        \"\"\"\n",
        "        # Convert BGR (OpenCV) to RGB (matplotlib)\n",
        "        img_rgb = cv2.cvtColor(bev_image, cv2.COLOR_BGR2RGB)\n",
        "        h, w = img_rgb.shape[:2]  # Get image dimensions\n",
        "\n",
        "        # Draw each bounding box and label\n",
        "        for label in yolo_labels:\n",
        "            class_id = int(label[0])  # Object class [0-3]\n",
        "\n",
        "            # Denormalize coordinates: [0,1] → pixel values\n",
        "            x_center, y_center = label[1] * w, label[2] * h\n",
        "            box_w, box_h = label[3] * w, label[4] * h\n",
        "\n",
        "            # Convert from center-size to corner format\n",
        "            x1 = int(x_center - box_w / 2)  # Top-left X\n",
        "            y1 = int(y_center - box_h / 2)  # Top-left Y\n",
        "            x2 = int(x_center + box_w / 2)  # Bottom-right X\n",
        "            y2 = int(y_center + box_h / 2)  # Bottom-right Y\n",
        "\n",
        "            # Draw rectangle (modifies image in-place)\n",
        "            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), self.colors[class_id], 2)\n",
        "\n",
        "            # Draw class label above box\n",
        "            cv2.putText(img_rgb, self.class_names[class_id], (x1, y1-10),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors[class_id], 2)\n",
        "\n",
        "        # Display using matplotlib\n",
        "        plt.figure(figsize=(12, 12))\n",
        "        plt.imshow(img_rgb)\n",
        "        plt.title('BEV Image with YOLO Annotations')\n",
        "        plt.axis('off')  # Hide axis ticks and labels\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_grid(self, bev_images, yolo_labels_list, num_cols=2):\n",
        "        \"\"\"\n",
        "        Visualize multiple BEV images in a grid layout.\n",
        "\n",
        "        Public interface method that delegates to _draw_grid for rendering.\n",
        "\n",
        "        Args:\n",
        "            bev_images: List of BEV images (numpy arrays)\n",
        "            yolo_labels_list: List of label lists (one per image)\n",
        "            num_cols: Number of columns in the grid layout\n",
        "\n",
        "        Returns:\n",
        "            Result from _draw_grid (typically None after plt.show())\n",
        "        \"\"\"\n",
        "        return self._draw_grid(bev_images, yolo_labels_list, num_cols)\n",
        "\n",
        "    def _draw_grid(self, bev_images, yolo_labels_list, num_cols=2):\n",
        "        \"\"\"\n",
        "        Internal method to render multiple BEV images with annotations in a grid.\n",
        "\n",
        "        Creates a matplotlib figure with subplots arranged in a grid, showing\n",
        "        multiple BEV images side-by-side for comparative analysis. This is useful\n",
        "        for:\n",
        "        - Dataset overview and quality assessment\n",
        "        - Comparing different scenes or time points\n",
        "        - Identifying patterns in object distribution\n",
        "        - Spotting preprocessing issues across samples\n",
        "\n",
        "        Args:\n",
        "            bev_images: List of numpy arrays (BGR images)\n",
        "            yolo_labels_list: List of annotation lists\n",
        "            num_cols: Number of columns in grid (default: 2)\n",
        "\n",
        "        Technical Details:\n",
        "            1. Grid Layout:\n",
        "               - Rows calculated as: ceil(num_samples / num_cols)\n",
        "               - Creates num_rows × num_cols subplot grid\n",
        "               - Unused subplots (if any) are hidden\n",
        "\n",
        "            2. Figure Sizing:\n",
        "               - Each subplot: 10×10 inches\n",
        "               - Total width: 10 * num_cols inches\n",
        "               - Total height: 10 * num_rows inches\n",
        "               - Large size ensures readability of small objects\n",
        "\n",
        "            3. Annotation Rendering:\n",
        "               - Boxes drawn but labels omitted (cleaner appearance in grid)\n",
        "               - Copy made of each image to avoid modifying originals\n",
        "               - Color coding preserved from class definitions\n",
        "\n",
        "            4. Subplot Titles:\n",
        "               - Format: \"Sample {index} ({count} objects)\"\n",
        "               - Provides quick object count per image\n",
        "               - Helps identify empty vs. crowded scenes\n",
        "\n",
        "            5. Edge Cases:\n",
        "               - Single image: axes becomes single object, not array\n",
        "               - Handles by converting to list when needed\n",
        "               - Extra subplots turned off to avoid empty panels\n",
        "        \"\"\"\n",
        "        num_samples = len(bev_images)\n",
        "\n",
        "        # Calculate grid dimensions\n",
        "        num_rows = (num_samples + num_cols - 1) // num_cols  # Ceiling division\n",
        "\n",
        "        # Create subplot grid\n",
        "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(10 * num_cols, 10 * num_rows))\n",
        "\n",
        "        # Handle single subplot case (axes is not an array)\n",
        "        if num_samples == 1:\n",
        "            axes = [axes]\n",
        "        else:\n",
        "            axes = axes.flatten()  # Convert 2D grid to 1D list\n",
        "\n",
        "        # Render each BEV image with annotations\n",
        "        for i in range(num_samples):\n",
        "            img = bev_images[i].copy()  # Copy to avoid modifying original\n",
        "            h, w = img.shape[:2]\n",
        "\n",
        "            # Draw bounding boxes for all objects in this image\n",
        "            for label in yolo_labels_list[i]:\n",
        "                class_id = int(label[0])\n",
        "\n",
        "                # Denormalize coordinates\n",
        "                x_center, y_center = label[1] * w, label[2] * h\n",
        "                box_w, box_h = label[3] * w, label[4] * h\n",
        "\n",
        "                # Convert to corner format\n",
        "                x1 = int(x_center - box_w / 2)\n",
        "                y1 = int(y_center - box_h / 2)\n",
        "                x2 = int(x_center + box_w / 2)\n",
        "                y2 = int(y_center + box_h / 2)\n",
        "\n",
        "                # Draw box (no label to reduce clutter in grid view)\n",
        "                cv2.rectangle(img, (x1, y1), (x2, y2), self.colors[class_id], 2)\n",
        "\n",
        "            # Convert BGR to RGB for matplotlib\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Display in subplot\n",
        "            axes[i].imshow(img_rgb)\n",
        "            axes[i].set_title(f'Sample {i} ({len(yolo_labels_list[i])} objects)')\n",
        "            axes[i].axis('off')  # Hide axis\n",
        "\n",
        "        # Hide unused subplots (if grid has more cells than images)\n",
        "        for i in range(num_samples, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        # Adjust spacing and display\n",
        "        plt.tight_layout()  # Reduce whitespace between subplots\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "H_y8VDkkO53Y",
      "metadata": {
        "id": "H_y8VDkkO53Y"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "BEVRasterizer: Convert 3D LiDAR point clouds to 2D Bird's Eye View (BEV) images.\n",
        "\n",
        "This module implements the core rasterization algorithm that projects 3D point clouds\n",
        "onto a 2D grid viewed from above (bird's eye perspective). The resulting BEV images\n",
        "encode height, intensity, and density information in three channels, making them\n",
        "suitable for 2D object detection models.\n",
        "\n",
        "Key Concepts:\n",
        "- Orthographic projection from 3D to 2D (top-down view)\n",
        "- Multi-channel encoding: height, intensity, density\n",
        "- Pixel-space quantization and accumulation\n",
        "- Normalization and perceptual enhancement\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BEVRasterizer:\n",
        "    \"\"\"\n",
        "    Rasterizer that converts 3D point clouds into 2D Bird's Eye View (BEV) images.\n",
        "\n",
        "    This class implements a projection algorithm that creates a top-down view of the\n",
        "    environment around the vehicle, encoding 3D information into a 2D image format\n",
        "    compatible with standard 2D object detection architectures like YOLO.\n",
        "\n",
        "    The BEV representation has several advantages:\n",
        "    - Preserves spatial relationships and distances (unlike perspective images)\n",
        "    - Eliminates scale variation with distance\n",
        "    - Provides consistent object sizes regardless of distance\n",
        "    - Suitable for accurate localization and planning\n",
        "\n",
        "    Attributes:\n",
        "        x_range: Tuple (min, max) for forward/backward extent (meters)\n",
        "        y_range: Tuple (min, max) for left/right extent (meters)\n",
        "        z_range: Tuple (min, max) for height extent (meters)\n",
        "        resolution: Meters per pixel in BEV image\n",
        "        width: Image width in pixels\n",
        "        height: Image height in pixels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_range=(-50, 50), y_range=(-50, 50), z_range=(-3, 5), resolution=0.1):\n",
        "        \"\"\"\n",
        "        Initialize BEV rasterizer with spatial parameters.\n",
        "\n",
        "        Args:\n",
        "            x_range: (min, max) coverage in forward direction (meters)\n",
        "                    Default: (-50, 50) = 100m range\n",
        "            y_range: (min, max) coverage in lateral direction (meters)\n",
        "                    Default: (-50, 50) = 100m range\n",
        "            z_range: (min, max) height range for normalization (meters)\n",
        "                    Default: (-3, 5) = 8m height range\n",
        "            resolution: Spatial resolution in meters per pixel\n",
        "                       Default: 0.1m = 10cm per pixel\n",
        "\n",
        "        Technical Details:\n",
        "            1. Resolution Trade-offs:\n",
        "               - Smaller (e.g., 0.05m): Higher detail, larger images, more memory\n",
        "               - Larger (e.g., 0.2m): Lower detail, smaller images, faster processing\n",
        "               - 0.1m (10cm) is common balance for autonomous driving\n",
        "\n",
        "            2. Image Dimensions:\n",
        "               - Width = (x_range[1] - x_range[0]) / resolution\n",
        "               - Height = (y_range[1] - y_range[0]) / resolution\n",
        "               - Example: 100m range / 0.1m resolution = 1000 pixels\n",
        "               - Typical BEV image: 1000×1000 pixels for 100m×100m area\n",
        "\n",
        "            3. Memory Footprint:\n",
        "               - 1000×1000×3 channels × 1 byte = ~3MB per BEV image\n",
        "               - Batch processing requires careful memory management\n",
        "        \"\"\"\n",
        "        self.x_range = x_range\n",
        "        self.y_range = y_range\n",
        "        self.z_range = z_range\n",
        "        self.resolution = resolution\n",
        "\n",
        "        # Calculate image dimensions from spatial extent and resolution\n",
        "        self.width = int((x_range[1] - x_range[0]) / resolution)   # Pixels in X direction\n",
        "        self.height = int((y_range[1] - y_range[0]) / resolution)  # Pixels in Y direction\n",
        "\n",
        "    def rasterize(self, points):\n",
        "        \"\"\"\n",
        "        Convert 3D point cloud to 2D Bird's Eye View image with three channels.\n",
        "\n",
        "        This method implements the core rasterization algorithm that projects 3D points\n",
        "        onto a 2D grid from above, accumulating height, intensity, and density information\n",
        "        for each pixel. The result is a 3-channel image encoding the 3D scene from above.\n",
        "\n",
        "        Args:\n",
        "            points: numpy.ndarray of shape (4, N) containing:\n",
        "                   - Row 0: X coordinates (forward) in meters\n",
        "                   - Row 1: Y coordinates (left) in meters\n",
        "                   - Row 2: Z coordinates (height) in meters\n",
        "                   - Row 3: Intensity values (LiDAR reflectivity)\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: BEV image of shape (height, width, 3) with dtype uint8\n",
        "                          - Channel 0 (Red): Height map (maximum Z value per pixel)\n",
        "                          - Channel 1 (Green): Intensity map (average intensity per pixel)\n",
        "                          - Channel 2 (Blue): Density map (point count per pixel)\n",
        "                          All values normalized to range [0, 255]\n",
        "\n",
        "        Technical Details:\n",
        "\n",
        "            **Algorithm Overview:**\n",
        "            1. Initialize three 2D accumulation maps (height, intensity, density)\n",
        "            2. Project 3D points to 2D pixel coordinates\n",
        "            3. Accumulate values for each pixel\n",
        "            4. Normalize and enhance channels\n",
        "            5. Stack into 3-channel RGB image\n",
        "\n",
        "            **Channel Semantics:**\n",
        "\n",
        "            • Height Map (Red Channel):\n",
        "              - Encodes maximum elevation at each (x,y) location\n",
        "              - Helps distinguish objects from ground plane\n",
        "              - Used for detecting vertical structures (vehicles, pedestrians)\n",
        "              - Formula: max(z) for all points projecting to same pixel\n",
        "\n",
        "            • Intensity Map (Green Channel):\n",
        "              - Encodes average LiDAR reflectivity\n",
        "              - Different materials have different reflectivities\n",
        "              - Helps distinguish object types (metal vs. fabric vs. vegetation)\n",
        "              - Formula: mean(intensity) for points in each pixel\n",
        "\n",
        "            • Density Map (Blue Channel):\n",
        "              - Encodes number of LiDAR points per pixel\n",
        "              - Indicates measurement confidence and proximity\n",
        "              - Higher density = closer objects or better visibility\n",
        "              - Formula: count(points) per pixel, log-normalized\n",
        "\n",
        "            **Coordinate Transformation:**\n",
        "            - 3D world coordinates (meters) → 2D pixel coordinates\n",
        "            - X_pixel = (X_world - X_min) / resolution\n",
        "            - Y_pixel = (Y_world - Y_min) / resolution\n",
        "            - Y-axis is flipped (image origin at top-left, world origin at center)\n",
        "\n",
        "            **Normalization Strategy:**\n",
        "            - Height: Linear normalization to [0,1], then sqrt for perceptual balance\n",
        "            - Intensity: Linear normalization to [0,1], then sqrt for contrast\n",
        "            - Density: Log normalization (log1p) for wide range, then power 0.3\n",
        "            - All channels scaled to [0, 255] for uint8 image format\n",
        "        \"\"\"\n",
        "        # Initialize three 2D accumulation maps for the BEV image\n",
        "        height_map = np.zeros((self.height, self.width), dtype=np.float32)     # Max height per pixel\n",
        "        intensity_map = np.zeros((self.height, self.width), dtype=np.float32)  # Sum of intensities\n",
        "        density_map = np.zeros((self.height, self.width), dtype=np.int32)      # Point count per pixel\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 1: Project 3D points to 2D pixel coordinates\n",
        "        # ============================================================\n",
        "\n",
        "        # Convert world coordinates (meters) to pixel coordinates\n",
        "        # X direction: forward/backward in world → horizontal in image\n",
        "        x_img = np.int32((points[0, :] - self.x_range[0]) / self.resolution)\n",
        "\n",
        "        # Y direction: left/right in world → vertical in image\n",
        "        y_img = np.int32((points[1, :] - self.y_range[0]) / self.resolution)\n",
        "\n",
        "        # Clamp pixel coordinates to image boundaries (handles edge cases)\n",
        "        x_img = np.clip(x_img, 0, self.width - 1)\n",
        "        y_img = np.clip(y_img, 0, self.height - 1)\n",
        "\n",
        "        # Flip Y-axis: image origin is top-left, world origin is center-bottom\n",
        "        # This makes the image appear with vehicle at bottom, forward direction up\n",
        "        y_img = self.height - 1 - y_img\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 2: Accumulate values for each pixel\n",
        "        # ============================================================\n",
        "\n",
        "        # Iterate through all points and update the three maps\n",
        "        # Note: Could be optimized with numpy operations, but loop is clear\n",
        "        for i in range(points.shape[1]):\n",
        "            x, y = x_img[i], y_img[i]  # Pixel coordinates for this point\n",
        "\n",
        "            # Height map: Keep maximum Z value (tallest point at this location)\n",
        "            height_map[y, x] = max(height_map[y, x], points[2, i])\n",
        "\n",
        "            # Intensity map: Accumulate intensity (will average later)\n",
        "            intensity_map[y, x] += points[3, i]\n",
        "\n",
        "            # Density map: Count number of points\n",
        "            density_map[y, x] += 1\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 3: Compute average intensity per pixel\n",
        "        # ============================================================\n",
        "\n",
        "        # Create mask for pixels with at least one point\n",
        "        mask = density_map > 0\n",
        "\n",
        "        # Convert accumulated intensity sum to average intensity\n",
        "        # Only for pixels with points (avoid division by zero)\n",
        "        intensity_map[mask] = intensity_map[mask] / density_map[mask]\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 4: Normalize and enhance each channel\n",
        "        # ============================================================\n",
        "\n",
        "        # --- Height Map Normalization ---\n",
        "        # Convert from absolute height (meters) to normalized [0, 1] range\n",
        "        # Formula: (height - min) / (max - min)\n",
        "        height_map = np.clip((height_map - self.z_range[0]) / (self.z_range[1] - self.z_range[0]), 0, 1)\n",
        "\n",
        "        # Apply gamma correction (power 0.5 = square root) for perceptual enhancement\n",
        "        # Brightens darker values, compresses brighter values\n",
        "        # Helps distinguish low-height objects from ground\n",
        "        height_map = np.power(height_map, 0.5)\n",
        "\n",
        "        # --- Intensity Map Normalization ---\n",
        "        # Normalize to [0, 1] range based on maximum intensity in this frame\n",
        "        # Add small epsilon (1e-6) to avoid division by zero\n",
        "        intensity_map = intensity_map / max(intensity_map.max(), 1e-6)\n",
        "\n",
        "        # Apply gamma correction for contrast enhancement\n",
        "        # Makes subtle reflectivity differences more visible\n",
        "        intensity_map = np.power(intensity_map, 0.5)\n",
        "\n",
        "        # --- Density Map Normalization ---\n",
        "        # Use logarithmic normalization for wide dynamic range\n",
        "        # log1p(x) = log(1 + x) handles zero values gracefully\n",
        "        # Compresses high densities while preserving low density variation\n",
        "        density_norm = np.log1p(density_map.astype(np.float32))\n",
        "\n",
        "        # Scale to [0, 1] range\n",
        "        density_norm = density_norm / max(density_norm.max(), 1e-6)\n",
        "\n",
        "        # Apply strong power transformation (0.3) to further compress range\n",
        "        # Emphasizes presence of points over exact count\n",
        "        density_norm = np.power(density_norm, 0.3)\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 5: Stack channels and convert to uint8 image format\n",
        "        # ============================================================\n",
        "\n",
        "        # Stack three normalized maps into RGB image (height, width, 3)\n",
        "        # Channel order: [height, intensity, density] → [R, G, B]\n",
        "        bev_image = np.stack([height_map, intensity_map, density_norm], axis=-1)\n",
        "\n",
        "        # Scale from [0, 1] float to [0, 255] uint8 for standard image format\n",
        "        return (bev_image * 255).astype(np.uint8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "aOXxCrrZQ852",
      "metadata": {
        "id": "aOXxCrrZQ852"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PointCloudProcessor: Transform and filter LiDAR point clouds for BEV processing.\n",
        "\n",
        "This module handles the spatial transformation of 3D point clouds from sensor coordinates\n",
        "to ego vehicle coordinates, and filters points to a region of interest (ROI) suitable for\n",
        "Bird's Eye View (BEV) image generation.\n",
        "\n",
        "Key Operations:\n",
        "- Load binary LiDAR data from nuScenes dataset\n",
        "- Apply rigid body transformations (rotation + translation)\n",
        "- Filter points to defined spatial boundaries\n",
        "- Prepare point clouds for 2D rasterization\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from nuscenes.utils.data_classes import LidarPointCloud\n",
        "from pyquaternion import Quaternion\n",
        "import os\n",
        "\n",
        "\n",
        "class PointCloudProcessor:\n",
        "    \"\"\"\n",
        "    Processor for loading, transforming, and filtering 3D LiDAR point clouds.\n",
        "\n",
        "    This class handles the coordinate transformations necessary to convert LiDAR point clouds\n",
        "    from sensor frame to ego vehicle frame, and filters points to a region of interest around\n",
        "    the vehicle suitable for autonomous driving perception tasks.\n",
        "\n",
        "    Attributes:\n",
        "        nusc: NuScenes dataset instance for accessing metadata and files\n",
        "        x_range: Tuple (min, max) defining forward/backward extent in meters\n",
        "        y_range: Tuple (min, max) defining left/right extent in meters\n",
        "        z_range: Tuple (min, max) defining up/down extent in meters\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nusc, x_range=(-50, 50), y_range=(-50, 50), z_range=(-3, 5)):\n",
        "        \"\"\"\n",
        "        Initialize the point cloud processor with spatial filtering boundaries.\n",
        "\n",
        "        Args:\n",
        "            nusc: NuScenes instance providing access to dataset\n",
        "            x_range: (min_x, max_x) in meters. X-axis points forward from vehicle.\n",
        "                    Default (-50, 50) = 100m total range, 50m ahead and behind\n",
        "            y_range: (min_y, max_y) in meters. Y-axis points left from vehicle.\n",
        "                    Default (-50, 50) = 100m total range, 50m on each side\n",
        "            z_range: (min_z, max_z) in meters. Z-axis points up from vehicle.\n",
        "                    Default (-3, 5) = 8m total height, 3m below to 5m above vehicle\n",
        "\n",
        "        Technical Details:\n",
        "            - Range selection impacts BEV image resolution and coverage area\n",
        "            - Typical autonomous vehicle perception: 50-100m forward, ±50m lateral\n",
        "            - Z-range filters ground points (below) and tall structures (above)\n",
        "            - Coordinate system: Right-handed with Z-up (ego vehicle frame)\n",
        "        \"\"\"\n",
        "        self.nusc = nusc\n",
        "        self.x_range = x_range\n",
        "        self.y_range = y_range\n",
        "        self.z_range = z_range\n",
        "\n",
        "    def load_and_transform(self, sample_data_token):\n",
        "        \"\"\"\n",
        "        Load LiDAR point cloud and transform from sensor frame to ego vehicle frame.\n",
        "\n",
        "        This method performs a rigid body transformation (rotation followed by translation)\n",
        "        to convert point cloud coordinates from the LiDAR sensor's local coordinate system\n",
        "        to the ego vehicle's coordinate system. This is essential for sensor fusion and\n",
        "        consistent spatial representation.\n",
        "\n",
        "        Args:\n",
        "            sample_data_token: UUID string identifying a specific LiDAR capture\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Point cloud in ego vehicle frame with shape (4, N) where:\n",
        "                - Row 0: X coordinates (forward) in meters\n",
        "                - Row 1: Y coordinates (left) in meters\n",
        "                - Row 2: Z coordinates (up) in meters\n",
        "                - Row 3: Intensity values (reflectivity)\n",
        "\n",
        "        Technical Details:\n",
        "            1. Data Loading:\n",
        "               - Retrieves file path from nuScenes metadata\n",
        "               - Loads binary .pcd.bin file containing float32 values\n",
        "               - Format: Interleaved [x, y, z, intensity] × N points\n",
        "\n",
        "            2. Coordinate Transformation Chain:\n",
        "               a) Points start in LiDAR sensor frame (LIDAR_TOP coordinate system)\n",
        "               b) Retrieve calibrated_sensor record containing extrinsics:\n",
        "                  - rotation: Quaternion representing sensor orientation relative to ego\n",
        "                  - translation: 3D vector [x, y, z] for sensor position relative to ego\n",
        "               c) Apply rotation: Points are rotated by converting quaternion to 3×3 matrix\n",
        "               d) Apply translation: Rotated points are shifted by translation vector\n",
        "               e) Result: Points in ego vehicle frame (centered at vehicle)\n",
        "\n",
        "            3. Transformation Mathematics:\n",
        "               - Rotation matrix R from quaternion q = [w, x, y, z]\n",
        "               - Translation vector t = [tx, ty, tz]\n",
        "               - Transformed point: p' = R × p + t\n",
        "               - Order matters: Rotate first, then translate\n",
        "\n",
        "            4. Ego Vehicle Frame Convention:\n",
        "               - Origin: Center of vehicle at ground level\n",
        "               - X-axis: Points forward (driving direction)\n",
        "               - Y-axis: Points left\n",
        "               - Z-axis: Points up\n",
        "               - Right-handed coordinate system\n",
        "        \"\"\"\n",
        "        # Get metadata record for this LiDAR capture\n",
        "        sample_data = self.nusc.get('sample_data', sample_data_token)\n",
        "\n",
        "        # Construct full path to binary point cloud file\n",
        "        pcl_path = os.path.join(self.nusc.dataroot, sample_data['filename'])\n",
        "\n",
        "        # Load point cloud from binary file (4×N array: x, y, z, intensity)\n",
        "        pc = LidarPointCloud.from_file(pcl_path)\n",
        "\n",
        "        # Retrieve sensor calibration (extrinsic parameters)\n",
        "        cs_record = self.nusc.get('calibrated_sensor', sample_data['calibrated_sensor_token'])\n",
        "\n",
        "        # ============================================================\n",
        "        # CRITICAL FIX: Keep LiDAR data in sensor frame\n",
        "        # ============================================================\n",
        "        # According to nuScenes documentation, the raw LiDAR data is already\n",
        "        # stored in the sensor (LIDAR_TOP) coordinate frame, NOT the ego vehicle frame.\n",
        "        #\n",
        "        # REASON FOR COMMENTING OUT TRANSFORMATION:\n",
        "        # The original code incorrectly transformed LiDAR data from sensor→ego frame,\n",
        "        # which caused misalignment with annotations. Since the data is already in the\n",
        "        # correct sensor frame, we should NOT apply any transformation here.\n",
        "        #\n",
        "        # Instead, annotations must be transformed TO the sensor frame (see DataPreprocessor.py)\n",
        "        # to match the coordinate system of the LiDAR point cloud.\n",
        "        #\n",
        "        # TRANSFORMATION STRATEGY:\n",
        "        # - LiDAR: Keep in sensor frame (no transformation needed)\n",
        "        # - Annotations: Transform from global → ego → sensor frame\n",
        "        # - Result: Both in same coordinate system for correct BEV projection\n",
        "        #\n",
        "        # Original transformation code (now disabled):\n",
        "        # pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
        "        # pc.translate(np.array(cs_record['translation']))\n",
        "        # ============================================================\n",
        "\n",
        "        # Return point cloud in sensor frame (4×N: x, y, z, intensity)\n",
        "        return pc.points\n",
        "\n",
        "    def filter_points(self, points):\n",
        "        \"\"\"\n",
        "        Filter point cloud to region of interest (ROI) using spatial boundaries.\n",
        "\n",
        "        Removes points outside the defined x, y, z ranges to focus on the relevant\n",
        "        area around the vehicle. This reduces computational load and focuses on\n",
        "        distances relevant for autonomous driving perception.\n",
        "\n",
        "        Args:\n",
        "            points: numpy.ndarray of shape (4, N) with rows [x, y, z, intensity]\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Filtered point cloud with shape (4, M) where M ≤ N\n",
        "                          Contains only points within the defined spatial boundaries\n",
        "\n",
        "        Technical Details:\n",
        "            1. Filtering Logic:\n",
        "               - Creates boolean mask using vectorized numpy comparisons\n",
        "               - Each condition checks one spatial dimension\n",
        "               - Combines conditions with logical AND (&) operator\n",
        "               - Applies mask to select only points satisfying all conditions\n",
        "\n",
        "            2. Spatial Filtering Rationale:\n",
        "               - X-range: Limits forward/backward perception distance\n",
        "                 • Too far: Low density, less relevant for immediate decisions\n",
        "                 • Too close: May miss distant objects\n",
        "               - Y-range: Limits lateral (side-to-side) perception\n",
        "                 • Typically symmetric around vehicle centerline\n",
        "               - Z-range: Removes ground and sky points\n",
        "                 • Below vehicle: Road surface, underground artifacts\n",
        "                 • Above vehicle: Bridges, buildings, sky noise\n",
        "\n",
        "            3. Performance Considerations:\n",
        "               - Vectorized operations using numpy for efficiency\n",
        "               - Typical filtering: 30K-40K → 10K-20K points (50-70% reduction)\n",
        "               - Reduces downstream processing time for rasterization\n",
        "\n",
        "            4. Impact on BEV:\n",
        "               - Filtered points determine BEV image coverage\n",
        "               - Points outside range are completely discarded\n",
        "               - Matches the spatial extent that will be rasterized\n",
        "        \"\"\"\n",
        "        # Create boolean mask: True for points inside ROI, False outside\n",
        "        mask = (\n",
        "            (points[0, :] >= self.x_range[0]) & (points[0, :] <= self.x_range[1]) &  # X bounds\n",
        "            (points[1, :] >= self.y_range[0]) & (points[1, :] <= self.y_range[1]) &  # Y bounds\n",
        "            (points[2, :] >= self.z_range[0]) & (points[2, :] <= self.z_range[1])    # Z bounds\n",
        "        )\n",
        "\n",
        "        # Apply mask to select only points within boundaries (fancy indexing)\n",
        "        return points[:, mask]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bCiW6hgmQ8XX",
      "metadata": {
        "id": "bCiW6hgmQ8XX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "YOLOAnnotationConverter: Transform 3D bounding boxes to YOLO format for BEV images.\n",
        "\n",
        "This module converts nuScenes 3D bounding box annotations (in meters with global coordinates)\n",
        "to YOLO-compatible 2D bounding box format (normalized pixel coordinates). It handles the\n",
        "projection from 3D world space to 2D BEV image space and maps nuScenes' 23 object categories\n",
        "to 4 simplified classes for object detection.\n",
        "\n",
        "Key Operations:\n",
        "- 3D bounding box → 2D bounding box projection (top-down view)\n",
        "- Coordinate transformation: meters → pixels → normalized [0,1]\n",
        "- Category mapping: 23 nuScenes classes → 4 detection classes\n",
        "- Validation: Ensures boxes are within image bounds\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class YOLOAnnotationConverter:\n",
        "    \"\"\"\n",
        "    Converter for transforming 3D bounding boxes to YOLO format for BEV images.\n",
        "\n",
        "    YOLO (You Only Look Once) format specifies 2D bounding boxes as:\n",
        "    <class_id> <x_center> <y_center> <width> <height>\n",
        "\n",
        "    where all spatial values are normalized to [0, 1] relative to image dimensions.\n",
        "\n",
        "    This class performs the geometric transformation from 3D boxes in world coordinates\n",
        "    to 2D boxes in BEV image coordinates, handling coordinate systems, resolution\n",
        "    conversion, and class label mapping.\n",
        "\n",
        "    Attributes:\n",
        "        image_width: BEV image width in pixels\n",
        "        image_height: BEV image height in pixels\n",
        "        x_range: Spatial extent in X direction (meters)\n",
        "        y_range: Spatial extent in Y direction (meters)\n",
        "        resolution: Meters per pixel\n",
        "        class_mapping: Dictionary mapping nuScenes categories to class IDs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_width, image_height, x_range=(-50, 50), y_range=(-50, 50), resolution=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the annotation converter with image and spatial parameters.\n",
        "\n",
        "        Args:\n",
        "            image_width: Width of BEV image in pixels (e.g., 1000)\n",
        "            image_height: Height of BEV image in pixels (e.g., 1000)\n",
        "            x_range: (min, max) spatial extent in X direction (meters)\n",
        "            y_range: (min, max) spatial extent in Y direction (meters)\n",
        "            resolution: Spatial resolution in meters per pixel (e.g., 0.1)\n",
        "\n",
        "        Technical Details:\n",
        "            - Image dimensions must match BEV rasterizer output\n",
        "            - Spatial ranges must match point cloud filtering ranges\n",
        "            - Resolution determines coordinate transformation accuracy\n",
        "        \"\"\"\n",
        "        self.image_width = image_width\n",
        "        self.image_height = image_height\n",
        "        self.x_range = x_range\n",
        "        self.y_range = y_range\n",
        "        self.resolution = resolution\n",
        "\n",
        "        # Map nuScenes' detailed taxonomy (23 classes) to simplified classes (4)\n",
        "        # Rationale: Reduces class imbalance, focuses on key autonomous driving objects\n",
        "        self.class_mapping = {\n",
        "            # Class 0: Cars (most common, ~60% of objects)\n",
        "            'vehicle.car': 0,\n",
        "            'vehicle.taxi': 0,  # Taxis are functionally similar to cars\n",
        "\n",
        "            # Class 1: Large Vehicles (trucks, buses, construction)\n",
        "            'vehicle.truck': 1,\n",
        "            'vehicle.bus.bendy': 1,  # Articulated buses\n",
        "            'vehicle.bus.rigid': 1,   # Standard buses\n",
        "            'vehicle.construction': 1,  # Bulldozers, cranes, etc.\n",
        "\n",
        "            # Class 2: Pedestrians (all types, critical for safety)\n",
        "            'human.pedestrian.adult': 2,\n",
        "            'human.pedestrian.child': 2,\n",
        "            'human.pedestrian.construction_worker': 2,\n",
        "            'human.pedestrian.police_officer': 2,\n",
        "\n",
        "            # Class 3: Two-wheeled Vehicles (vulnerable road users)\n",
        "            'vehicle.bicycle': 3,\n",
        "            'vehicle.motorcycle': 3\n",
        "\n",
        "            # Note: Other nuScenes classes not included:\n",
        "            # - vehicle.trailer, vehicle.emergency.*: Uncommon\n",
        "            # - movable_object.*: Traffic cones, barriers (static)\n",
        "            # - animal: Very rare in nuScenes dataset\n",
        "        }\n",
        "\n",
        "    def convert_annotation(self, box_translation, box_size, category_name):\n",
        "        \"\"\"\n",
        "        Convert a single 3D bounding box annotation to YOLO format for BEV image.\n",
        "\n",
        "        Transforms a 3D box defined in world coordinates (meters) to a 2D box in BEV\n",
        "        image coordinates (normalized [0,1]). The conversion involves:\n",
        "        1. Category filtering and mapping\n",
        "        2. 3D→2D projection (top-down, uses only X and Y coordinates)\n",
        "        3. Coordinate transformation (meters → pixels → normalized)\n",
        "        4. Validation (ensure box is within image bounds)\n",
        "\n",
        "        Args:\n",
        "            box_translation: [x, y, z] center of 3D bounding box in ego frame (meters)\n",
        "            box_size: [width, length, height] dimensions of 3D box (meters)\n",
        "                     Note: nuScenes uses [width, length, height] order\n",
        "            category_name: String category from nuScenes taxonomy (e.g., 'vehicle.car')\n",
        "\n",
        "        Returns:\n",
        "            List [class_id, x_center, y_center, width, height] in YOLO format, or None if:\n",
        "            - Category is not in class_mapping (filtered out)\n",
        "            - Box center is outside image bounds\n",
        "            - Box dimensions are invalid (≤0 or >1 after normalization)\n",
        "\n",
        "        Technical Details:\n",
        "\n",
        "            **Coordinate System Transformation:**\n",
        "\n",
        "            1. Input Space (3D world, ego vehicle frame):\n",
        "               - Origin: Center of vehicle\n",
        "               - X-axis: Forward (driving direction)\n",
        "               - Y-axis: Left\n",
        "               - Z-axis: Up (discarded in BEV)\n",
        "               - Units: Meters\n",
        "\n",
        "            2. Intermediate Space (2D pixel coordinates):\n",
        "               - Origin: Top-left corner\n",
        "               - X-axis: Right (horizontal)\n",
        "               - Y-axis: Down (vertical)\n",
        "               - Units: Pixels\n",
        "               - Transformation: x_px = (x_m - x_min) / resolution\n",
        "\n",
        "            3. Output Space (YOLO normalized coordinates):\n",
        "               - Origin: Top-left corner\n",
        "               - X-axis: Right, range [0, 1]\n",
        "               - Y-axis: Down, range [0, 1]\n",
        "               - Units: Fraction of image dimensions\n",
        "               - Transformation: x_norm = x_px / image_width\n",
        "\n",
        "            **Y-Axis Flip:**\n",
        "            - World coordinates: Y increases leftward\n",
        "            - Image coordinates: Y increases downward\n",
        "            - Requires flip: y_img = image_height - 1 - y_world_to_px\n",
        "\n",
        "            **Size Interpretation:**\n",
        "            - box_size[0]: Width (lateral extent, X direction in world)\n",
        "            - box_size[1]: Length (longitudinal extent, Y direction in world)\n",
        "            - In BEV: Width maps to image X, Length maps to image Y\n",
        "            - Z dimension (height) is discarded for 2D projection\n",
        "\n",
        "            **Validation Logic:**\n",
        "            - Filters unknown categories (returns None)\n",
        "            - Rejects boxes with center outside [0, 1] range\n",
        "            - Rejects boxes with invalid dimensions (too large or non-positive)\n",
        "            - Clamps final values to [0, 1] as safety measure\n",
        "        \"\"\"\n",
        "        # ============================================================\n",
        "        # STEP 1: Filter by category (return None if not in mapping)\n",
        "        # ============================================================\n",
        "        if category_name not in self.class_mapping:\n",
        "            return None  # Skip classes we're not detecting (e.g., trafficcone)\n",
        "\n",
        "        class_id = self.class_mapping[category_name]  # Map to simplified class [0-3]\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 2: Convert 3D box center to 2D pixel coordinates\n",
        "        # ============================================================\n",
        "\n",
        "        # Transform X coordinate: world meters → pixel index\n",
        "        # X in world frame (forward) → X in image (horizontal)\n",
        "        x_center = (box_translation[0] - self.x_range[0]) / self.resolution\n",
        "\n",
        "        # Transform Y coordinate: world meters → pixel index (before flip)\n",
        "        y_center = (box_translation[1] - self.y_range[0]) / self.resolution\n",
        "\n",
        "        # Flip Y-axis: image origin is top-left, world origin is center\n",
        "        # In world: Y positive = left; In image: Y positive = down\n",
        "        y_center = self.image_height - 1 - y_center\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 3: Convert 3D box size to 2D pixel dimensions\n",
        "        # ============================================================\n",
        "\n",
        "        # Width: box_size[0] is lateral extent (X direction)\n",
        "        width = box_size[0] / self.resolution\n",
        "\n",
        "        # Height: box_size[1] is longitudinal extent (Y direction)\n",
        "        # Note: \"height\" in YOLO 2D means vertical extent in image, not Z\n",
        "        height = box_size[1] / self.resolution\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 4: Normalize to [0, 1] range (YOLO format requirement)\n",
        "        # ============================================================\n",
        "\n",
        "        # Normalize center coordinates\n",
        "        x_norm = x_center / self.image_width   # Fraction of image width\n",
        "        y_norm = y_center / self.image_height  # Fraction of image height\n",
        "\n",
        "        # Normalize dimensions\n",
        "        w_norm = width / self.image_width\n",
        "        h_norm = height / self.image_height\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 5: Validate box (reject invalid or out-of-bounds boxes)\n",
        "        # ============================================================\n",
        "\n",
        "        # Check if dimensions are valid (positive and not too large)\n",
        "        if w_norm <= 0 or h_norm <= 0 or w_norm > 1 or h_norm > 1:\n",
        "            return None  # Invalid box size\n",
        "\n",
        "        # Check if center is within image bounds\n",
        "        if x_norm < 0 or x_norm > 1 or y_norm < 0 or y_norm > 1:\n",
        "            return None  # Box center outside image\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 6: Return YOLO format annotation\n",
        "        # ============================================================\n",
        "\n",
        "        # Format: [class_id, x_center, y_center, width, height]\n",
        "        # All spatial values normalized to [0, 1]\n",
        "        # Clamp as final safety check (should rarely trigger after validation)\n",
        "        return [class_id,\n",
        "                np.clip(x_norm, 0, 1),\n",
        "                np.clip(y_norm, 0, 1),\n",
        "                np.clip(w_norm, 0, 1),\n",
        "                np.clip(h_norm, 0, 1)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5nj7CMOHO9Sj",
      "metadata": {
        "id": "5nj7CMOHO9Sj"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DataPreprocessor: Orchestrate the full nuScenes → YOLO BEV dataset conversion pipeline.\n",
        "\n",
        "This is the main preprocessing orchestrator that combines all preprocessing components to\n",
        "convert the raw nuScenes dataset into a YOLO-compatible BEV object detection dataset.\n",
        "\n",
        "Pipeline Overview:\n",
        "1. Load 3D LiDAR point cloud → PointCloudProcessor\n",
        "2. Transform to ego vehicle frame → PointCloudProcessor\n",
        "3. Filter to region of interest → PointCloudProcessor\n",
        "4. Rasterize to BEV image → BEVRasterizer\n",
        "5. Transform 3D annotations to 2D YOLO format → YOLOAnnotationConverter\n",
        "6. Save paired images and labels to disk\n",
        "\n",
        "Output Structure:\n",
        "    build/data/preprocessed/\n",
        "    ├── images/\n",
        "    │   ├── scene-0001_<token>.png\n",
        "    │   ├── scene-0002_<token>.png\n",
        "    │   └── ...\n",
        "    └── labels/\n",
        "        ├── scene-0001_<token>.txt\n",
        "        ├── scene-0002_<token>.txt\n",
        "        └── ...\n",
        "\"\"\"\n",
        "\n",
        "#from Preprocessing.PointCloudProcessor import PointCloudProcessor\n",
        "#from Preprocessing.BEVRasterizer import BEVRasterizer\n",
        "#from Preprocessing.YOLOAnnotationConverter import YOLOAnnotationConverter\n",
        "from nuscenes.utils.data_classes import Box\n",
        "from pyquaternion import Quaternion\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from pathlib import Path\n",
        "#from Globals import PREPROCESSED_ROOT\n",
        "\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    Main preprocessing pipeline orchestrator for nuScenes → YOLO BEV conversion.\n",
        "\n",
        "    This class coordinates the entire preprocessing workflow, managing the flow of data\n",
        "    through each processing stage and handling file I/O for the output dataset.\n",
        "\n",
        "    The preprocessor creates a dataset suitable for training YOLO models on BEV images,\n",
        "    with each sample consisting of:\n",
        "    - A 3-channel BEV image (PNG format, 1000×1000 pixels by default)\n",
        "    - A corresponding YOLO format label file (TXT format, one box per line)\n",
        "\n",
        "    Attributes:\n",
        "        nusc: NuScenes dataset instance\n",
        "        pc_processor: PointCloudProcessor for loading and filtering point clouds\n",
        "        rasterizer: BEVRasterizer for converting point clouds to images\n",
        "        converter: YOLOAnnotationConverter for transforming annotations\n",
        "        output_root: Root directory for preprocessed dataset\n",
        "        images_dir: Directory for BEV images\n",
        "        labels_dir: Directory for YOLO labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nusc):\n",
        "        \"\"\"\n",
        "        Initialize the data preprocessor with all required components.\n",
        "\n",
        "        Args:\n",
        "            nusc: NuScenes instance providing access to the raw dataset\n",
        "\n",
        "        Technical Details:\n",
        "            - Creates instances of all preprocessing components with compatible parameters\n",
        "            - Sets up output directory structure (creates if doesn't exist)\n",
        "            - Ensures image dimensions match between rasterizer and annotation converter\n",
        "        \"\"\"\n",
        "        self.nusc = nusc\n",
        "\n",
        "        # Initialize preprocessing components with default parameters\n",
        "        self.pc_processor = PointCloudProcessor(nusc)\n",
        "        self.rasterizer = BEVRasterizer()\n",
        "\n",
        "        # Pass rasterizer dimensions to ensure coordinate transformation consistency\n",
        "        self.converter = YOLOAnnotationConverter(\n",
        "            self.rasterizer.width,\n",
        "            self.rasterizer.height\n",
        "        )\n",
        "\n",
        "        # Setup output directory structure\n",
        "        self.output_root = Path(PREPROCESSED_ROOT)\n",
        "        self.images_dir = self.output_root / 'images'  # BEV PNG files\n",
        "        self.labels_dir = self.output_root / 'labels'  # YOLO TXT files\n",
        "\n",
        "        # Create directories (parents=True creates intermediate dirs, exist_ok ignores if exists)\n",
        "        self.images_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.labels_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def process_all_samples(self):\n",
        "        \"\"\"\n",
        "        Process all samples in the nuScenes dataset and save as YOLO BEV dataset.\n",
        "\n",
        "        This is the main processing loop that iterates through every sample (timestamp)\n",
        "        in the nuScenes dataset, generates a BEV image and YOLO annotations, and saves\n",
        "        them to disk in a format suitable for training object detection models.\n",
        "\n",
        "        Returns:\n",
        "            int: Total number of samples processed\n",
        "\n",
        "        Processing Pipeline per Sample:\n",
        "            1. Load LiDAR point cloud\n",
        "            2. Transform to ego vehicle frame\n",
        "            3. Filter to region of interest\n",
        "            4. Rasterize to BEV image\n",
        "            5. Transform all 3D annotations to 2D YOLO format\n",
        "            6. Save image and labels with matching filenames\n",
        "\n",
        "        Technical Details:\n",
        "\n",
        "            **Coordinate Frame Transformations:**\n",
        "            The preprocessing involves two key coordinate transformations:\n",
        "\n",
        "            a) Point Cloud Transformation (handled by PointCloudProcessor):\n",
        "               - Sensor frame → Ego vehicle frame\n",
        "               - Applied via calibrated_sensor record (rotation + translation)\n",
        "\n",
        "            b) Annotation Transformation (handled in this method):\n",
        "               - Global frame → Ego vehicle frame\n",
        "               - Required because annotations are in global coordinates\n",
        "               - Uses ego_pose to transform: T_ego^global\n",
        "               - Inverse transformation: box_ego = T_ego^global^-1 * box_global\n",
        "\n",
        "            **Why Two Transformations?**\n",
        "            - Point clouds: Stored in sensor frame, need ego frame alignment\n",
        "            - Annotations: Stored in global frame, need ego frame alignment\n",
        "            - Both must be in same frame (ego) for consistent BEV projection\n",
        "\n",
        "            **File Naming Convention:**\n",
        "            - Format: {scene_name}_{sample_token}.{ext}\n",
        "            - Example: scene-0061_3e8750f331d7499e9b5123e9eb70f2e2.png\n",
        "            - Ensures unique names and preserves scene context\n",
        "            - Matching names for image-label pairs enable automatic pairing\n",
        "\n",
        "            **YOLO Label Format (per line in .txt file):**\n",
        "            <class_id> <x_center> <y_center> <width> <height>\n",
        "            - class_id: Integer [0-3]\n",
        "            - All other values: Floats in range [0.0, 1.0]\n",
        "            - Space-separated values\n",
        "            - One line per object\n",
        "\n",
        "            **Performance Considerations:**\n",
        "            - v1.0-mini: ~400 samples, takes ~5-10 minutes to process\n",
        "            - Full dataset: ~40,000 samples, takes hours\n",
        "            - Each BEV image: ~3MB (1000×1000×3)\n",
        "            - Total output: ~1.2GB for mini, ~120GB for full dataset\n",
        "        \"\"\"\n",
        "        # Get total number of samples to process (v1.0-mini has ~400)\n",
        "        total_samples = len(self.nusc.sample)\n",
        "\n",
        "        # ============================================================\n",
        "        # Main Processing Loop: Iterate through all samples\n",
        "        # ============================================================\n",
        "        for sample_idx in range(total_samples):\n",
        "            # Get the sample record (represents one timestamp across all sensors)\n",
        "            sample = self.nusc.sample[sample_idx]\n",
        "\n",
        "            # Extract LIDAR_TOP token (primary 3D sensor for BEV generation)\n",
        "            lidar_token = sample['data']['LIDAR_TOP']\n",
        "\n",
        "            # --------------------------------------------------------\n",
        "            # STAGE 1: Generate BEV Image from Point Cloud\n",
        "            # --------------------------------------------------------\n",
        "\n",
        "            # Load and transform point cloud to ego vehicle frame\n",
        "            points = self.pc_processor.load_and_transform(lidar_token)\n",
        "\n",
        "            # Filter to region of interest (removes distant/irrelevant points)\n",
        "            filtered_points = self.pc_processor.filter_points(points)\n",
        "\n",
        "            # Rasterize 3D points to 2D BEV image (height, intensity, density channels)\n",
        "            bev_image = self.rasterizer.rasterize(filtered_points)\n",
        "\n",
        "            # --------------------------------------------------------\n",
        "            # STAGE 2: Transform Annotations to YOLO Format\n",
        "            # --------------------------------------------------------\n",
        "\n",
        "            # ============================================================\n",
        "            # COORDINATE FRAME TRANSFORMATION: Global → Ego → Sensor\n",
        "            # ============================================================\n",
        "            # PROBLEM: nuScenes stores data in different coordinate frames:\n",
        "            # - LiDAR point clouds: Sensor frame (LIDAR_TOP coordinate system)\n",
        "            # - Annotations (bounding boxes): Global frame (world coordinates)\n",
        "            #\n",
        "            # SOLUTION: Transform annotations to match LiDAR's sensor frame\n",
        "            # Transformation chain: Global → Ego Vehicle → Sensor\n",
        "            #\n",
        "            # WHY THIS IS NECESSARY:\n",
        "            # For proper BEV projection, both point cloud and annotations must be\n",
        "            # in the same coordinate system. Since we keep LiDAR in sensor frame\n",
        "            # (see PointCloudProcessor.py), we must transform annotations TO sensor frame.\n",
        "            # ============================================================\n",
        "\n",
        "            # Get ego vehicle pose for this timestamp (needed for global→ego transform)\n",
        "            sample_data = self.nusc.get('sample_data', lidar_token)\n",
        "            ego_pose = self.nusc.get('ego_pose', sample_data['ego_pose_token'])\n",
        "\n",
        "            # Get sensor calibration for ego→sensor transformation\n",
        "            cs_record = self.nusc.get('calibrated_sensor', sample_data['calibrated_sensor_token'])\n",
        "\n",
        "            # Process all annotations (3D bounding boxes) for this sample\n",
        "            yolo_labels = []\n",
        "            for ann_token in sample['anns']:\n",
        "                # Get annotation metadata (stored in global/world coordinates)\n",
        "                ann = self.nusc.get('sample_annotation', ann_token)\n",
        "\n",
        "                # Create 3D box object from annotation (currently in global frame)\n",
        "                box = Box(\n",
        "                    ann['translation'],  # [x, y, z] center in global frame\n",
        "                    ann['size'],         # [width, length, height] dimensions\n",
        "                    Quaternion(ann['rotation'])  # Orientation quaternion\n",
        "                )\n",
        "\n",
        "                # --------------------------------------------------------\n",
        "                # TRANSFORMATION STEP 1 & 2: Global Frame → Ego Vehicle Frame\n",
        "                # --------------------------------------------------------\n",
        "                # The ego vehicle frame has its origin at the center of the vehicle\n",
        "                # with X=forward, Y=left, Z=up (right-handed system)\n",
        "\n",
        "                # Step 1: Translate by negative ego position (center on ego)\n",
        "                box.translate(-np.array(ego_pose['translation']))\n",
        "\n",
        "                # Step 2: Rotate by inverse ego orientation (align with ego axes)\n",
        "                box.rotate(Quaternion(ego_pose['rotation']).inverse)\n",
        "\n",
        "                # --------------------------------------------------------\n",
        "                # TRANSFORMATION STEP 3 & 4: Ego Frame → Sensor Frame\n",
        "                # --------------------------------------------------------\n",
        "                # CRITICAL FIX: This transformation was missing in the original code,\n",
        "                # causing misalignment between LiDAR points and bounding boxes.\n",
        "                #\n",
        "                # The sensor frame (LIDAR_TOP) has a different origin and orientation\n",
        "                # than the ego frame, so we must apply the sensor calibration transform.\n",
        "\n",
        "                # Step 3: Translate by negative sensor position (center on sensor)\n",
        "                box.translate(-np.array(cs_record['translation']))\n",
        "\n",
        "                # Step 4: Rotate by inverse sensor orientation (align with sensor axes)\n",
        "                box.rotate(Quaternion(cs_record['rotation']).inverse)\n",
        "\n",
        "                # --------------------------------------------------------\n",
        "                # AXIS-ALIGNED BOUNDING BOX COMPUTATION\n",
        "                # --------------------------------------------------------\n",
        "                # PROBLEM: Standard YOLO format only supports axis-aligned bounding boxes\n",
        "                # (no rotation angle). However, our 3D boxes are rotated in 3D space.\n",
        "                #\n",
        "                # SOLUTION: Compute the minimum axis-aligned bounding box (AABB) that\n",
        "                # fully contains the rotated 3D box when viewed from above (BEV).\n",
        "                #\n",
        "                # WHY THIS IS NECESSARY:\n",
        "                # - A car at 45° has a larger footprint in axis-aligned coordinates\n",
        "                # - Using original width/length would create boxes that don't fully\n",
        "                #   contain the rotated object\n",
        "                # - AABB ensures the box properly encloses the object at any angle\n",
        "                #\n",
        "                # ALGORITHM:\n",
        "                # 1. Get all 8 corners of the rotated 3D box\n",
        "                # 2. Find min/max X and Y coordinates (top-down projection)\n",
        "                # 3. Compute new center and dimensions from these extents\n",
        "                # --------------------------------------------------------\n",
        "\n",
        "                # Get all 8 corners of the rotated 3D box (3×8 array: x, y, z for each corner)\n",
        "                corners = box.corners()\n",
        "\n",
        "                # For BEV (top-down view), we only need X and Y coordinates (Z is discarded)\n",
        "                # Find the min/max extents in X and Y to create the smallest axis-aligned box\n",
        "                x_min, x_max = corners[0, :].min(), corners[0, :].max()\n",
        "                y_min, y_max = corners[1, :].min(), corners[1, :].max()\n",
        "\n",
        "                # Compute axis-aligned center (midpoint of extents)\n",
        "                # Z coordinate remains unchanged (height doesn't affect top-down projection)\n",
        "                aa_center = np.array([(x_min + x_max) / 2, (y_min + y_max) / 2, box.center[2]])\n",
        "\n",
        "                # Compute axis-aligned dimensions (extent ranges)\n",
        "                # Width = X extent, Length = Y extent, Height = original Z dimension\n",
        "                aa_size = np.array([x_max - x_min, y_max - y_min, box.wlh[2]])\n",
        "\n",
        "                # Convert 3D axis-aligned box (sensor frame) to 2D YOLO annotation (BEV image)\n",
        "                yolo_label = self.converter.convert_annotation(\n",
        "                    aa_center,          # Axis-aligned box center in sensor frame\n",
        "                    aa_size,            # Axis-aligned box dimensions [width, length, height]\n",
        "                    ann['category_name']  # Object category for class mapping\n",
        "                )\n",
        "\n",
        "                # Add to list if valid (converter returns None for invalid/filtered boxes)\n",
        "                if yolo_label:\n",
        "                    yolo_labels.append(yolo_label)\n",
        "\n",
        "            # --------------------------------------------------------\n",
        "            # STAGE 3: Save Image and Labels to Disk\n",
        "            # --------------------------------------------------------\n",
        "\n",
        "            # Generate unique filename from scene name and sample token\n",
        "            scene_name = self.nusc.get('scene', sample['scene_token'])['name']\n",
        "            filename = f\"{scene_name}_{sample['token']}\"\n",
        "\n",
        "            # Save BEV image as PNG (3-channel RGB, uint8)\n",
        "            image_path = self.images_dir / f\"{filename}.png\"\n",
        "            cv2.imwrite(str(image_path), bev_image)\n",
        "\n",
        "            # Save YOLO labels as TXT (one line per object)\n",
        "            label_path = self.labels_dir / f\"{filename}.txt\"\n",
        "            with open(label_path, 'w') as f:\n",
        "                for label in yolo_labels:\n",
        "                    # Format: <class> <x> <y> <w> <h>\n",
        "                    f.write(f\"{label[0]} {label[1]} {label[2]} {label[3]} {label[4]}\\n\")\n",
        "\n",
        "        # Return total number of processed samples\n",
        "        return total_samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "916EFcz5RJw0",
      "metadata": {
        "id": "916EFcz5RJw0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ModelInitializer: Initialize YOLO model with pretrained weights.\n",
        "\n",
        "This module handles loading the YOLOv12 model with pretrained COCO weights for\n",
        "transfer learning. It downloads weights if needed and prepares the model for training.\n",
        "\n",
        "Key Operations:\n",
        "- Load YOLOv12 with pretrained weights\n",
        "- Validate model architecture\n",
        "- Prepare for transfer learning\n",
        "\"\"\"\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "#from Globals import MODELS_ROOT\n",
        "\n",
        "\n",
        "class ModelInitializer:\n",
        "    \"\"\"\n",
        "    Initializer for YOLO models with transfer learning support.\n",
        "\n",
        "    This class handles model instantiation, pretrained weight loading, and\n",
        "    configuration for fine-tuning on the BEV detection task.\n",
        "\n",
        "    Attributes:\n",
        "        model_size: YOLO model size variant ('n', 's', 'm', 'l')\n",
        "        pretrained: Whether to use pretrained weights\n",
        "        models_dir: Directory for storing model weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_size='s', pretrained=True):\n",
        "        \"\"\"\n",
        "        Initialize the model initializer.\n",
        "\n",
        "        Args:\n",
        "            model_size: Model variant - 'n' (nano), 's' (small), 'm' (medium), 'l' (large)\n",
        "            pretrained: Load COCO pretrained weights (True) or random initialization (False)\n",
        "\n",
        "        Technical Details:\n",
        "            - 's' (small) provides good balance of speed and accuracy\n",
        "            - Pretrained weights improve convergence and final performance\n",
        "            - Models are cached in build/models/ directory\n",
        "        \"\"\"\n",
        "        self.model_size = model_size\n",
        "        self.pretrained = pretrained\n",
        "        self.models_dir = Path(MODELS_ROOT)\n",
        "        self.models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"\n",
        "        Initialize YOLO model with optional pretrained weights.\n",
        "\n",
        "        Downloads pretrained weights if needed and creates a YOLO model instance\n",
        "        ready for training on the BEV detection task.\n",
        "\n",
        "        Returns:\n",
        "            YOLO model instance configured for training\n",
        "\n",
        "        Technical Details:\n",
        "\n",
        "            **Model Variants:**\n",
        "            - YOLOv12n: 1.9M params, fastest, lowest accuracy\n",
        "            - YOLOv12s: 9.1M params, balanced (recommended)\n",
        "            - YOLOv12m: 23.8M params, higher accuracy, slower\n",
        "            - YOLOv12l: 52.6M params, highest accuracy, slowest\n",
        "\n",
        "            **Transfer Learning:**\n",
        "            - Pretrained weights are from COCO dataset (80 classes)\n",
        "            - Detection head will be replaced with 4-class head automatically\n",
        "            - Backbone and neck preserve learned features\n",
        "            - Significant speedup in convergence vs random initialization\n",
        "\n",
        "            **Weight Management:**\n",
        "            - Weights downloaded to build/models/ on first use\n",
        "            - Subsequent runs use cached weights\n",
        "            - Network connection required only for first download\n",
        "        \"\"\"\n",
        "        print(f\"\\nInitializing YOLOv12{self.model_size} model...\")\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 1: Construct model name\n",
        "        # ============================================================\n",
        "        if self.pretrained:\n",
        "            # Pretrained weights format: yolo12s.pt\n",
        "            model_name = f'yolo12{self.model_size}.pt'\n",
        "            print(f\"  Loading with COCO pretrained weights\")\n",
        "        else:\n",
        "            # Architecture config format: yolo12s.yaml\n",
        "            model_name = f'yolo12{self.model_size}.yaml'\n",
        "            print(f\"  Random initialization (no pretrained weights)\")\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 2: Initialize model\n",
        "        # ============================================================\n",
        "        # YOLO() automatically downloads weights if not found\n",
        "        model = YOLO(model_name)\n",
        "\n",
        "        # ============================================================\n",
        "        # STEP 3: Print model info\n",
        "        # ============================================================\n",
        "        # Count total parameters\n",
        "        total_params = sum(p.numel() for p in model.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
        "\n",
        "        print(f\"  Model: YOLOv12{self.model_size}\")\n",
        "        print(f\"  Total parameters: {total_params / 1e6:.2f}M\")\n",
        "        print(f\"  Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
        "        print(f\"  Pretrained: {self.pretrained}\")\n",
        "\n",
        "        return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f2vzMS_xRQJu",
      "metadata": {
        "id": "f2vzMS_xRQJu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TrainingOrchestrator: Orchestrate two-stage YOLO training pipeline.\n",
        "\n",
        "This module manages the complete training workflow including warm-up stage with\n",
        "frozen backbone and fine-tuning stage with all layers trainable. It handles\n",
        "hyperparameter configuration, training execution, and checkpoint management.\n",
        "\n",
        "Key Operations:\n",
        "- Configure training hyperparameters for each stage\n",
        "- Execute two-stage training (warm-up + fine-tuning)\n",
        "- Manage checkpoints and logging\n",
        "- Monitor training progress\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "#from Globals import RUNS_ROOT\n",
        "\n",
        "\n",
        "class TrainingOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrator for two-stage YOLO training pipeline.\n",
        "\n",
        "    Manages the complete training workflow with separate warm-up and fine-tuning\n",
        "    stages to optimize transfer learning from COCO to BEV detection task.\n",
        "\n",
        "    Attributes:\n",
        "        model: YOLO model instance\n",
        "        dataset_yaml: Path to dataset configuration file\n",
        "        runs_dir: Directory for training outputs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, dataset_yaml):\n",
        "        \"\"\"\n",
        "        Initialize the training orchestrator.\n",
        "\n",
        "        Args:\n",
        "            model: YOLO model instance from ModelInitializer\n",
        "            dataset_yaml: Path to dataset.yaml configuration file\n",
        "\n",
        "        Technical Details:\n",
        "            - Training outputs saved to build/runs/\n",
        "            - Separate directories for each training stage\n",
        "            - Tensorboard logs automatically generated\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.dataset_yaml = str(dataset_yaml)\n",
        "        self.runs_dir = Path(RUNS_ROOT)\n",
        "        self.runs_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def train_stage1_warmup(self, epochs=50, batch_size=16, img_size=1000):\n",
        "        \"\"\"\n",
        "        Stage 1: Warm-up training with frozen backbone.\n",
        "\n",
        "        Trains only the detection head while keeping the backbone frozen. This\n",
        "        allows the head to adapt to the new domain (BEV detection) without\n",
        "        disrupting pretrained feature extraction.\n",
        "\n",
        "        Args:\n",
        "            epochs: Number of training epochs (default: 50)\n",
        "            batch_size: Batch size (default: 16, adjust based on GPU memory)\n",
        "            img_size: Input image size (default: 1000 to match BEV resolution)\n",
        "\n",
        "        Returns:\n",
        "            Training results object from YOLO\n",
        "\n",
        "        Technical Details:\n",
        "\n",
        "            **Freezing Strategy:**\n",
        "            - freeze=10: Freezes first 10 layers (backbone)\n",
        "            - Detection head and neck remain trainable\n",
        "            - Prevents catastrophic forgetting of low-level features\n",
        "\n",
        "            **Learning Rate:**\n",
        "            - lr0=0.01: Higher initial LR acceptable since only head trains\n",
        "            - Cosine annealing reduces LR over epochs\n",
        "            - Warm-up helps stabilize early training\n",
        "\n",
        "            **Data Augmentation:**\n",
        "            - Mosaic: Combines 4 images for better small object detection\n",
        "            - MixUp: Blends images to improve generalization\n",
        "            - Geometric: Rotation, translation, scale for robustness\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"STAGE 1: WARM-UP TRAINING (Frozen Backbone)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # ============================================================\n",
        "        # Configure Stage 1 hyperparameters\n",
        "        # ============================================================\n",
        "        config = {\n",
        "            # Dataset\n",
        "            'data': self.dataset_yaml,\n",
        "            'imgsz': img_size,\n",
        "            'batch': batch_size,\n",
        "\n",
        "            # Training duration\n",
        "            'epochs': epochs,\n",
        "\n",
        "            # Optimizer\n",
        "            'optimizer': 'AdamW',\n",
        "            'lr0': 0.01,          # Initial learning rate\n",
        "            'lrf': 0.01,          # Final LR (fraction of lr0)\n",
        "            'momentum': 0.937,\n",
        "            'weight_decay': 0.0005,\n",
        "\n",
        "            # Learning rate schedule\n",
        "            'cos_lr': True,       # Cosine annealing\n",
        "            'warmup_epochs': 3,\n",
        "            'warmup_momentum': 0.8,\n",
        "            'warmup_bias_lr': 0.1,\n",
        "\n",
        "            # Freezing\n",
        "            'freeze': 10,         # Freeze first 10 layers (backbone)\n",
        "\n",
        "            # Data augmentation\n",
        "            'degrees': 15.0,      # Rotation\n",
        "            'translate': 0.1,     # Translation\n",
        "            'scale': 0.5,         # Scale\n",
        "            'fliplr': 0.5,        # Horizontal flip\n",
        "            'mosaic': 0.5,        # Mosaic augmentation\n",
        "            'mixup': 0.0,         # MixUp augmentation\n",
        "\n",
        "            # Hardware\n",
        "            'device': 0,          # GPU 0 (use 'cpu' for CPU training)\n",
        "            'workers': 2,         # DataLoader workers\n",
        "            'cache': False,       # Cache images for faster training\n",
        "            'amp': True,          # Automatic mixed precision\n",
        "\n",
        "            # Logging and saving\n",
        "            'project': str(self.runs_dir / 'detect'),\n",
        "            'name': 'stage1_warmup',\n",
        "            'exist_ok': True,\n",
        "            'save': True,\n",
        "            'save_period': 10,    # Save checkpoint every 10 epochs\n",
        "            'plots': True,\n",
        "            'verbose': True,\n",
        "\n",
        "            # Other\n",
        "            'seed': 42,\n",
        "            'deterministic': True,\n",
        "            'val': True,          # Run validation\n",
        "        }\n",
        "\n",
        "        # ============================================================\n",
        "        # Execute training\n",
        "        # ============================================================\n",
        "        print(f\"\\nStarting Stage 1 training...\")\n",
        "        print(f\"  Epochs: {epochs}\")\n",
        "        print(f\"  Batch size: {batch_size}\")\n",
        "        print(f\"  Image size: {img_size}\")\n",
        "        print(f\"  Frozen layers: 10 (backbone)\")\n",
        "\n",
        "        results = self.model.train(**config)\n",
        "\n",
        "        print(f\"\\n✓ Stage 1 complete\")\n",
        "        print(f\"  Results saved to: {results.save_dir}\")\n",
        "        print(f\"  Best weights: {results.save_dir}/weights/best.pt\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def train_stage2_finetune(self, stage1_weights_path, epochs=150, batch_size=16, img_size=1000):\n",
        "        \"\"\"\n",
        "        Stage 2: Fine-tuning with all layers trainable.\n",
        "\n",
        "        Unfreezes all layers and trains end-to-end with reduced learning rate.\n",
        "        This allows the entire network to adapt to BEV-specific features while\n",
        "        maintaining learned representations.\n",
        "\n",
        "        Args:\n",
        "            stage1_weights_path: Path to best weights from Stage 1\n",
        "            epochs: Number of training epochs (default: 150)\n",
        "            batch_size: Batch size (default: 16)\n",
        "            img_size: Input image size (default: 1000)\n",
        "\n",
        "        Returns:\n",
        "            Training results object from YOLO\n",
        "\n",
        "        Technical Details:\n",
        "\n",
        "            **Unfreezing Strategy:**\n",
        "            - freeze=0: All layers trainable\n",
        "            - Lower LR prevents catastrophic forgetting\n",
        "            - Allows fine-grained adaptation to BEV domain\n",
        "\n",
        "            **Learning Rate:**\n",
        "            - lr0=0.001: 10x lower than Stage 1\n",
        "            - Prevents disrupting learned features\n",
        "            - Enables careful fine-tuning\n",
        "\n",
        "            **Early Stopping:**\n",
        "            - patience=50: Stop if no improvement for 50 epochs\n",
        "            - Prevents overfitting\n",
        "            - Saves computational resources\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"STAGE 2: FINE-TUNING (All Layers Trainable)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # ============================================================\n",
        "        # Load best model from Stage 1\n",
        "        # ============================================================\n",
        "        from ultralytics import YOLO\n",
        "        model = YOLO(stage1_weights_path)\n",
        "        print(f\"Loaded Stage 1 weights from: {stage1_weights_path}\")\n",
        "\n",
        "        # ============================================================\n",
        "        # Configure Stage 2 hyperparameters\n",
        "        # ============================================================\n",
        "        config = {\n",
        "            # Dataset\n",
        "            'data': self.dataset_yaml,\n",
        "            'imgsz': img_size,\n",
        "            'batch': batch_size,\n",
        "\n",
        "            # Training duration\n",
        "            'epochs': epochs,\n",
        "\n",
        "            # Optimizer\n",
        "            'optimizer': 'AdamW',\n",
        "            'lr0': 0.001,         # Lower LR for fine-tuning\n",
        "            'lrf': 0.001,\n",
        "            'momentum': 0.937,\n",
        "            'weight_decay': 0.0005,\n",
        "\n",
        "            # Learning rate schedule\n",
        "            'cos_lr': True,\n",
        "            'warmup_epochs': 0,   # No warmup needed\n",
        "\n",
        "            # Freezing\n",
        "            'freeze': 0,          # Unfreeze all layers\n",
        "\n",
        "            # Data augmentation (same as Stage 1)\n",
        "            'degrees': 15.0,\n",
        "            'translate': 0.1,\n",
        "            'scale': 0.5,\n",
        "            'fliplr': 0.5,\n",
        "            'mosaic': 1.0,\n",
        "            'mixup': 0.1,\n",
        "\n",
        "            # Hardware\n",
        "            'device': 0,\n",
        "            'workers': 8,\n",
        "\n",
        "            # Logging and saving\n",
        "            'project': str(self.runs_dir / 'detect'),\n",
        "            'name': 'stage2_finetune',\n",
        "            'exist_ok': True,\n",
        "            'save': True,\n",
        "            'save_period': 10,\n",
        "            'plots': True,\n",
        "            'verbose': True,\n",
        "\n",
        "            # Early stopping\n",
        "            'patience': 50,       # Stop if no improvement for 50 epochs\n",
        "\n",
        "            # Other\n",
        "            'seed': 42,\n",
        "            'deterministic': True,\n",
        "            'val': True,\n",
        "        }\n",
        "\n",
        "        # ============================================================\n",
        "        # Execute training\n",
        "        # ============================================================\n",
        "        print(f\"\\nStarting Stage 2 training...\")\n",
        "        print(f\"  Epochs: {epochs}\")\n",
        "        print(f\"  Batch size: {batch_size}\")\n",
        "        print(f\"  Image size: {img_size}\")\n",
        "        print(f\"  All layers trainable\")\n",
        "\n",
        "        results = model.train(**config)\n",
        "\n",
        "        print(f\"\\n✓ Stage 2 complete\")\n",
        "        print(f\"  Results saved to: {results.save_dir}\")\n",
        "        print(f\"  Best weights: {results.save_dir}/weights/best.pt\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def train_full_pipeline(self, stage1_epochs=50, stage2_epochs=150, batch_size=16):\n",
        "        \"\"\"\n",
        "        Execute complete two-stage training pipeline.\n",
        "\n",
        "        Convenience method that runs both training stages sequentially,\n",
        "        automatically passing the best weights from Stage 1 to Stage 2.\n",
        "\n",
        "        Args:\n",
        "            stage1_epochs: Epochs for warm-up stage (default: 50)\n",
        "            stage2_epochs: Epochs for fine-tuning stage (default: 150)\n",
        "            batch_size: Batch size for both stages (default: 16)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (stage1_results, stage2_results)\n",
        "        \"\"\"\n",
        "        # Stage 1: Warm-up\n",
        "        stage1_results = self.train_stage1_warmup(\n",
        "            epochs=stage1_epochs,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Get best weights from Stage 1\n",
        "        stage1_best = Path(stage1_results.save_dir) / 'weights' / 'best.pt'\n",
        "\n",
        "        # Stage 2: Fine-tuning\n",
        "        stage2_results = self.train_stage2_finetune(\n",
        "            stage1_weights_path=stage1_best,\n",
        "            epochs=stage2_epochs,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        return stage1_results, stage2_results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "Zt64IgKfRcnE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt64IgKfRcnE",
        "outputId": "f4ca704d-a1d0-4a3a-a6c7-aad0076376a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Initializing YOLOv12s model...\n",
            "  Loading with COCO pretrained weights\n",
            "  Model: YOLOv12s\n",
            "  Total parameters: 9.29M\n",
            "  Trainable parameters: 0.00M\n",
            "  Pretrained: True\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from nuscenes.nuscenes import NuScenes\n",
        "dataset_yaml_path = Path(DATA_ROOT) / 'dataset.yaml'\n",
        "\n",
        "#We create the model first, so it's available for all training stages\n",
        "model_initializer = ModelInitializer(model_size='s', pretrained=True)\n",
        "model = model_initializer.initialize()\n",
        "\n",
        "trainer = TrainingOrchestrator(model, dataset_yaml_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def download():\n",
        "\n",
        "    downloader = DataDownloader()\n",
        "    if not downloader.check_and_prompt():\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def validate():\n",
        "    validator = DataValidator()\n",
        "    if not validator.validate():\n",
        "        print(\"Dataset validation failed\")\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def inspect():\n",
        "    nusc = NuScenes(version=NUSCENES_VERSION, dataroot=NUSCENES_ROOT, verbose=False)\n",
        "\n",
        "    inspector = RawDataInspector(nusc)\n",
        "    inspector.list_scenes()\n",
        "    inspector.visualize_sample()\n",
        "    inspector.visualize_sample_data()\n",
        "    inspector.visualize_annotation()\n",
        "\n",
        "    inspector = RawDataInspector(nusc)\n",
        "\n",
        "    sample = nusc.sample[10]\n",
        "    lidar_token = sample['data']['LIDAR_TOP']\n",
        "\n",
        "    pc_info = inspector.inspect_point_cloud(lidar_token)\n",
        "    annotations = inspector.inspect_annotations(sample['token'])\n",
        "    inspector.visualize_3d_scene(sample['token'])\n",
        "\n",
        "    print(\"\\n=== Inspection Point 1: Raw Data ===\")\n",
        "    print(f\"\\nPoint Cloud: {pc_info['num_points']} points\")\n",
        "    print(f\"X: [{pc_info['x_range'][0]:.2f}, {pc_info['x_range'][1]:.2f}] m\")\n",
        "    print(f\"Y: [{pc_info['y_range'][0]:.2f}, {pc_info['y_range'][1]:.2f}] m\")\n",
        "    print(f\"Z: [{pc_info['z_range'][0]:.2f}, {pc_info['z_range'][1]:.2f}] m\")\n",
        "    return nusc\n",
        "def preprocess(nusc):\n",
        "\n",
        "    print(\"\\n=== Preprocessing Stage ===\")\n",
        "    preprocessor = DataPreprocessor(nusc)\n",
        "    total = preprocessor.process_all_samples()\n",
        "    print(f\"Processed {total} samples\")\n",
        "\n",
        "    print(\"\\n=== Inspection Point 2: Preprocessed Data ===\")\n",
        "    bev_inspector = BEVInspector()\n",
        "    bev_images, yolo_labels_list = bev_inspector.load_samples(4)\n",
        "    bev_inspector.visualize_grid(bev_images, yolo_labels_list, num_cols=2)\n",
        "\n",
        "\n",
        "    print(\"\\n=== Data Preparation Stage ===\")\n",
        "    splitter = DataSplitter(train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
        "    splits = splitter.split()\n",
        "\n",
        "    config_generator = DatasetConfigGenerator()\n",
        "    config_generator.generate(splits, dataset_yaml_path)\n",
        "    print(\"✓ Data preparation complete\")\n",
        "    return splits\n",
        "def train():\n",
        "    print(\"\\n=== Training Stage ===\")\n",
        "\n",
        "    #user_input = input(\"\\nProceed with training? This will take several hours. (y/n): \")\n",
        "    #if user_input.lower() != 'y':\n",
        "    #    print(\"Training skipped by user\")\n",
        "    #    return 0\n",
        "\n",
        "    stage1_results, stage2_results = trainer.train_full_pipeline(\n",
        "        stage1_epochs=50,\n",
        "        stage2_epochs=150,\n",
        "        batch_size=16\n",
        "    )\n",
        "\n",
        "    best_model_path = Path(stage2_results.save_dir) / 'weights' / 'best.pt'\n",
        "    print(f\"\\n✓ Training complete. Best model: {best_model_path}\")\n",
        "def train_warmup(epochs=50, batch_size=16):\n",
        "  print(\"\\n=== Training Stage - Warmup only ===\")\n",
        "  # Stage 1: Warm-up\n",
        "  stage1_results = trainer.train_stage1_warmup(\n",
        "      epochs=epochs,\n",
        "      batch_size=batch_size\n",
        "  )\n",
        "\n",
        "  # Get best weights from Stage 1\n",
        "  stage1_best = Path(stage1_results.save_dir) / 'weights' / 'best.pt'\n",
        "\n",
        "  return stage1_best\n",
        "def train_finetune(stage1_best, epochs=150, batch_size=16):\n",
        "  print(\"\\n=== Training Stage - Fine-tuning ===\")\n",
        "  # Stage 2: Fine-tuning\n",
        "  stage2_results = trainer.train_stage2_finetune(\n",
        "      stage1_weights_path=stage1_best,\n",
        "      epochs=epochs,\n",
        "      batch_size=batch_size\n",
        "  )\n",
        "  return stage2_results\n",
        "def evaluate(best_model_path):\n",
        "    print(\"\\n=== Evaluation Stage ===\")\n",
        "\n",
        "    evaluator = ModelEvaluator(best_model_path, dataset_yaml_path)\n",
        "    results = evaluator.evaluate()\n",
        "    evaluator.print_metrics(results)\n",
        "\n",
        "    analyzer = PerformanceAnalyzer()\n",
        "    summary = analyzer.compute_performance_summary(results)\n",
        "\n",
        "    labels_dir = Path(PREPROCESSED_ROOT) / 'labels'\n",
        "    analyzer.analyze_class_distribution(labels_dir)\n",
        "\n",
        "    visualizer = ResultsVisualizer()\n",
        "    #test_images_dir = Path(splits['test']['images'][0]).parent\n",
        "    test_images_dir = '/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/data/split_data/images/test'\n",
        "    visualizer.visualize_predictions(evaluator.model, test_images_dir, num_samples=10)\n",
        "    visualizer.generate_performance_report(results)\n",
        "\n",
        "    print(\"\\n✓ Pipeline complete\")\n",
        "    print(f\"mAP@0.5: {summary['overall']['mAP_50']:.4f}\")\n",
        "    print(f\"mAP@0.5:0.95: {summary['overall']['mAP_50_95']:.4f}\")\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    exit(main())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OwJPfCmESehT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwJPfCmESehT",
        "outputId": "ad3056c6-dc0a-4550-9e79-3301521088ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nuScenes dataset not found at: build/data/raw/v1.0-mini\n",
            "\n",
            "Download instructions:\n",
            "1. Visit: https://www.nuscenes.org/nuscenes#download\n",
            "2. Download v1.0-mini (4 GB)\n",
            "3. Extract to: build/data/raw/v1.0-mini\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Now, let's execute everything in sequence\n",
        "#First, we'll download\n",
        "download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OWa6I7K-cZWH",
      "metadata": {
        "id": "OWa6I7K-cZWH"
      },
      "outputs": [],
      "source": [
        "#Now we Validate\n",
        "validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31cIBJX_cfHi",
      "metadata": {
        "id": "31cIBJX_cfHi"
      },
      "outputs": [],
      "source": [
        "#Next, we Inspect\n",
        "inspect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MXV3l7Enchx6",
      "metadata": {
        "id": "MXV3l7Enchx6"
      },
      "outputs": [],
      "source": [
        "#Now we run preprocess\n",
        "preprocess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bg4O2dJbckZk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg4O2dJbckZk",
        "outputId": "935598f1-375f-445c-c1d4-0cd3a803adb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 1: WARM-UP TRAINING (Frozen Backbone)\n",
            "================================================================================\n",
            "\n",
            "Starting Stage 1 training...\n",
            "  Epochs: 50\n",
            "  Batch size: 16\n",
            "  Image size: 1000\n",
            "  Frozen layers: 10 (backbone)\n",
            "Ultralytics 8.3.231 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/data/dataset.yaml, degrees=15.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1000, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo12s.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=stage1_warmup, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
            "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
            "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n",
            " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n",
            " 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n",
            " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 21        [14, 17, 20]  1    820956  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
            "YOLOv12s summary: 272 layers, 9,254,684 parameters, 9,254,668 gradients, 21.5 GFLOPs\n",
            "\n",
            "Transferred 685/691 items from pretrained weights\n",
            "Freezing layer 'model.0.conv.weight'\n",
            "Freezing layer 'model.0.bn.weight'\n",
            "Freezing layer 'model.0.bn.bias'\n",
            "Freezing layer 'model.1.conv.weight'\n",
            "Freezing layer 'model.1.bn.weight'\n",
            "Freezing layer 'model.1.bn.bias'\n",
            "Freezing layer 'model.2.cv1.conv.weight'\n",
            "Freezing layer 'model.2.cv1.bn.weight'\n",
            "Freezing layer 'model.2.cv1.bn.bias'\n",
            "Freezing layer 'model.2.cv2.conv.weight'\n",
            "Freezing layer 'model.2.cv2.bn.weight'\n",
            "Freezing layer 'model.2.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.3.conv.weight'\n",
            "Freezing layer 'model.3.bn.weight'\n",
            "Freezing layer 'model.3.bn.bias'\n",
            "Freezing layer 'model.4.cv1.conv.weight'\n",
            "Freezing layer 'model.4.cv1.bn.weight'\n",
            "Freezing layer 'model.4.cv1.bn.bias'\n",
            "Freezing layer 'model.4.cv2.conv.weight'\n",
            "Freezing layer 'model.4.cv2.bn.weight'\n",
            "Freezing layer 'model.4.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.5.conv.weight'\n",
            "Freezing layer 'model.5.bn.weight'\n",
            "Freezing layer 'model.5.bn.bias'\n",
            "Freezing layer 'model.6.cv1.conv.weight'\n",
            "Freezing layer 'model.6.cv1.bn.weight'\n",
            "Freezing layer 'model.6.cv1.bn.bias'\n",
            "Freezing layer 'model.6.cv2.conv.weight'\n",
            "Freezing layer 'model.6.cv2.bn.weight'\n",
            "Freezing layer 'model.6.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.0.0.attn.qkv.conv.weight'\n",
            "Freezing layer 'model.6.m.0.0.attn.qkv.bn.weight'\n",
            "Freezing layer 'model.6.m.0.0.attn.qkv.bn.bias'\n",
            "Freezing layer 'model.6.m.0.0.attn.proj.conv.weight'\n",
            "Freezing layer 'model.6.m.0.0.attn.proj.bn.weight'\n",
            "Freezing layer 'model.6.m.0.0.attn.proj.bn.bias'\n",
            "Freezing layer 'model.6.m.0.0.attn.pe.conv.weight'\n",
            "Freezing layer 'model.6.m.0.0.attn.pe.bn.weight'\n",
            "Freezing layer 'model.6.m.0.0.attn.pe.bn.bias'\n",
            "Freezing layer 'model.6.m.0.0.mlp.0.conv.weight'\n",
            "Freezing layer 'model.6.m.0.0.mlp.0.bn.weight'\n",
            "Freezing layer 'model.6.m.0.0.mlp.0.bn.bias'\n",
            "Freezing layer 'model.6.m.0.0.mlp.1.conv.weight'\n",
            "Freezing layer 'model.6.m.0.0.mlp.1.bn.weight'\n",
            "Freezing layer 'model.6.m.0.0.mlp.1.bn.bias'\n",
            "Freezing layer 'model.6.m.0.1.attn.qkv.conv.weight'\n",
            "Freezing layer 'model.6.m.0.1.attn.qkv.bn.weight'\n",
            "Freezing layer 'model.6.m.0.1.attn.qkv.bn.bias'\n",
            "Freezing layer 'model.6.m.0.1.attn.proj.conv.weight'\n",
            "Freezing layer 'model.6.m.0.1.attn.proj.bn.weight'\n",
            "Freezing layer 'model.6.m.0.1.attn.proj.bn.bias'\n",
            "Freezing layer 'model.6.m.0.1.attn.pe.conv.weight'\n",
            "Freezing layer 'model.6.m.0.1.attn.pe.bn.weight'\n",
            "Freezing layer 'model.6.m.0.1.attn.pe.bn.bias'\n",
            "Freezing layer 'model.6.m.0.1.mlp.0.conv.weight'\n",
            "Freezing layer 'model.6.m.0.1.mlp.0.bn.weight'\n",
            "Freezing layer 'model.6.m.0.1.mlp.0.bn.bias'\n",
            "Freezing layer 'model.6.m.0.1.mlp.1.conv.weight'\n",
            "Freezing layer 'model.6.m.0.1.mlp.1.bn.weight'\n",
            "Freezing layer 'model.6.m.0.1.mlp.1.bn.bias'\n",
            "Freezing layer 'model.6.m.1.0.attn.qkv.conv.weight'\n",
            "Freezing layer 'model.6.m.1.0.attn.qkv.bn.weight'\n",
            "Freezing layer 'model.6.m.1.0.attn.qkv.bn.bias'\n",
            "Freezing layer 'model.6.m.1.0.attn.proj.conv.weight'\n",
            "Freezing layer 'model.6.m.1.0.attn.proj.bn.weight'\n",
            "Freezing layer 'model.6.m.1.0.attn.proj.bn.bias'\n",
            "Freezing layer 'model.6.m.1.0.attn.pe.conv.weight'\n",
            "Freezing layer 'model.6.m.1.0.attn.pe.bn.weight'\n",
            "Freezing layer 'model.6.m.1.0.attn.pe.bn.bias'\n",
            "Freezing layer 'model.6.m.1.0.mlp.0.conv.weight'\n",
            "Freezing layer 'model.6.m.1.0.mlp.0.bn.weight'\n",
            "Freezing layer 'model.6.m.1.0.mlp.0.bn.bias'\n",
            "Freezing layer 'model.6.m.1.0.mlp.1.conv.weight'\n",
            "Freezing layer 'model.6.m.1.0.mlp.1.bn.weight'\n",
            "Freezing layer 'model.6.m.1.0.mlp.1.bn.bias'\n",
            "Freezing layer 'model.6.m.1.1.attn.qkv.conv.weight'\n",
            "Freezing layer 'model.6.m.1.1.attn.qkv.bn.weight'\n",
            "Freezing layer 'model.6.m.1.1.attn.qkv.bn.bias'\n",
            "Freezing layer 'model.6.m.1.1.attn.proj.conv.weight'\n",
            "Freezing layer 'model.6.m.1.1.attn.proj.bn.weight'\n",
            "Freezing layer 'model.6.m.1.1.attn.proj.bn.bias'\n",
            "Freezing layer 'model.6.m.1.1.attn.pe.conv.weight'\n",
            "Freezing layer 'model.6.m.1.1.attn.pe.bn.weight'\n",
            "Freezing layer 'model.6.m.1.1.attn.pe.bn.bias'\n",
            "Freezing layer 'model.6.m.1.1.mlp.0.conv.weight'\n",
            "Freezing layer 'model.6.m.1.1.mlp.0.bn.weight'\n",
            "Freezing layer 'model.6.m.1.1.mlp.0.bn.bias'\n",
            "Freezing layer 'model.6.m.1.1.mlp.1.conv.weight'\n",
            "Freezing layer 'model.6.m.1.1.mlp.1.bn.weight'\n",
            "Freezing layer 'model.6.m.1.1.mlp.1.bn.bias'\n",
            "Freezing layer 'model.7.conv.weight'\n",
            "Freezing layer 'model.7.bn.weight'\n",
            "Freezing layer 'model.7.bn.bias'\n",
            "Freezing layer 'model.8.cv1.conv.weight'\n",
            "Freezing layer 'model.8.cv1.bn.weight'\n",
            "Freezing layer 'model.8.cv1.bn.bias'\n",
            "Freezing layer 'model.8.cv2.conv.weight'\n",
            "Freezing layer 'model.8.cv2.bn.weight'\n",
            "Freezing layer 'model.8.cv2.bn.bias'\n",
            "Freezing layer 'model.8.m.0.0.attn.qkv.conv.weight'\n",
            "Freezing layer 'model.8.m.0.0.attn.qkv.bn.weight'\n",
            "Freezing layer 'model.8.m.0.0.attn.qkv.bn.bias'\n",
            "Freezing layer 'model.8.m.0.0.attn.proj.conv.weight'\n",
            "Freezing layer 'model.8.m.0.0.attn.proj.bn.weight'\n",
            "Freezing layer 'model.8.m.0.0.attn.proj.bn.bias'\n",
            "Freezing layer 'model.8.m.0.0.attn.pe.conv.weight'\n",
            "Freezing layer 'model.8.m.0.0.attn.pe.bn.weight'\n",
            "Freezing layer 'model.8.m.0.0.attn.pe.bn.bias'\n",
            "Freezing layer 'model.8.m.0.0.mlp.0.conv.weight'\n",
            "Freezing layer 'model.8.m.0.0.mlp.0.bn.weight'\n",
            "Freezing layer 'model.8.m.0.0.mlp.0.bn.bias'\n",
            "Freezing layer 'model.8.m.0.0.mlp.1.conv.weight'\n",
            "Freezing layer 'model.8.m.0.0.mlp.1.bn.weight'\n",
            "Freezing layer 'model.8.m.0.0.mlp.1.bn.bias'\n",
            "Freezing layer 'model.8.m.0.1.attn.qkv.conv.weight'\n",
            "Freezing layer 'model.8.m.0.1.attn.qkv.bn.weight'\n",
            "Freezing layer 'model.8.m.0.1.attn.qkv.bn.bias'\n",
            "Freezing layer 'model.8.m.0.1.attn.proj.conv.weight'\n",
            "Freezing layer 'model.8.m.0.1.attn.proj.bn.weight'\n",
            "Freezing layer 'model.8.m.0.1.attn.proj.bn.bias'\n",
            "Freezing layer 'model.8.m.0.1.attn.pe.conv.weight'\n",
            "Freezing layer 'model.8.m.0.1.attn.pe.bn.weight'\n",
            "Freezing layer 'model.8.m.0.1.attn.pe.bn.bias'\n",
            "Freezing layer 'model.8.m.0.1.mlp.0.conv.weight'\n",
            "Freezing layer 'model.8.m.0.1.mlp.0.bn.weight'\n",
            "Freezing layer 'model.8.m.0.1.mlp.0.bn.bias'\n",
            "Freezing layer 'model.8.m.0.1.mlp.1.conv.weight'\n",
            "Freezing layer 'model.8.m.0.1.mlp.1.bn.weight'\n",
            "Freezing layer 'model.8.m.0.1.mlp.1.bn.bias'\n",
            "Freezing layer 'model.8.m.1.0.attn.qkv.conv.weight'\n",
            "Freezing layer 'model.8.m.1.0.attn.qkv.bn.weight'\n",
            "Freezing layer 'model.8.m.1.0.attn.qkv.bn.bias'\n",
            "Freezing layer 'model.8.m.1.0.attn.proj.conv.weight'\n",
            "Freezing layer 'model.8.m.1.0.attn.proj.bn.weight'\n",
            "Freezing layer 'model.8.m.1.0.attn.proj.bn.bias'\n",
            "Freezing layer 'model.8.m.1.0.attn.pe.conv.weight'\n",
            "Freezing layer 'model.8.m.1.0.attn.pe.bn.weight'\n",
            "Freezing layer 'model.8.m.1.0.attn.pe.bn.bias'\n",
            "Freezing layer 'model.8.m.1.0.mlp.0.conv.weight'\n",
            "Freezing layer 'model.8.m.1.0.mlp.0.bn.weight'\n",
            "Freezing layer 'model.8.m.1.0.mlp.0.bn.bias'\n",
            "Freezing layer 'model.8.m.1.0.mlp.1.conv.weight'\n",
            "Freezing layer 'model.8.m.1.0.mlp.1.bn.weight'\n",
            "Freezing layer 'model.8.m.1.0.mlp.1.bn.bias'\n",
            "Freezing layer 'model.8.m.1.1.attn.qkv.conv.weight'\n",
            "Freezing layer 'model.8.m.1.1.attn.qkv.bn.weight'\n",
            "Freezing layer 'model.8.m.1.1.attn.qkv.bn.bias'\n",
            "Freezing layer 'model.8.m.1.1.attn.proj.conv.weight'\n",
            "Freezing layer 'model.8.m.1.1.attn.proj.bn.weight'\n",
            "Freezing layer 'model.8.m.1.1.attn.proj.bn.bias'\n",
            "Freezing layer 'model.8.m.1.1.attn.pe.conv.weight'\n",
            "Freezing layer 'model.8.m.1.1.attn.pe.bn.weight'\n",
            "Freezing layer 'model.8.m.1.1.attn.pe.bn.bias'\n",
            "Freezing layer 'model.8.m.1.1.mlp.0.conv.weight'\n",
            "Freezing layer 'model.8.m.1.1.mlp.0.bn.weight'\n",
            "Freezing layer 'model.8.m.1.1.mlp.0.bn.bias'\n",
            "Freezing layer 'model.8.m.1.1.mlp.1.conv.weight'\n",
            "Freezing layer 'model.8.m.1.1.mlp.1.bn.weight'\n",
            "Freezing layer 'model.8.m.1.1.mlp.1.bn.bias'\n",
            "Freezing layer 'model.21.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "WARNING ⚠️ imgsz=[1000] must be multiple of max stride 32, updating to [1024]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.6±0.1 ms, read: 31.1±7.5 MB/s, size: 55.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/data/split_data/labels/train.cache... 322 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 322/322 454.9Kit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.8±0.3 ms, read: 25.7±13.1 MB/s, size: 58.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/data/split_data/labels/val.cache... 41 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 41/41 33.8Kit/s 0.0s\n",
            "Plotting labels to /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50      8.09G      3.349      4.406      2.242        105       1024: 100% ━━━━━━━━━━━━ 21/21 1.0s/it 21.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7s/it 3.3s\n",
            "                   all         41       1379    0.00094    0.00574   0.000574   7.16e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50       6.3G       2.95      3.077      1.916         86       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.2s/it 2.4s\n",
            "                   all         41       1379          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50      6.31G      2.857      2.972      1.847         74       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4s/it 2.8s\n",
            "                   all         41       1379   0.000536    0.00948   0.000285     0.0001\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50      6.31G       2.77      2.851       1.81         58       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.6s/it 3.2s\n",
            "                   all         41       1379    0.00861      0.106    0.00576    0.00197\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50      6.31G      2.699      2.762      1.769         36       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.6s/it 3.2s\n",
            "                   all         41       1379     0.0203      0.143     0.0157    0.00461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50      7.19G       2.68      2.724      1.767         68       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.6s/it 3.2s\n",
            "                   all         41       1379      0.327      0.155     0.0344     0.0143\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50      7.19G      2.656      2.683      1.733        132       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7s/it 3.3s\n",
            "                   all         41       1379      0.337     0.0956     0.0719     0.0334\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50      7.21G      2.606      2.624      1.707        102       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7s/it 3.4s\n",
            "                   all         41       1379      0.383      0.169     0.0994     0.0388\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50      7.21G      2.525      2.517       1.68        101       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.7s/it 3.4s\n",
            "                   all         41       1379      0.185      0.164      0.123     0.0507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50      7.21G      2.481      2.486      1.624         36       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.5s/it 3.0s\n",
            "                   all         41       1379      0.259      0.127      0.126     0.0428\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50      7.24G      2.458      2.391      1.641         29       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4s/it 2.9s\n",
            "                   all         41       1379      0.285      0.137      0.121     0.0513\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50      7.24G      2.402      2.398      1.598         16       1024: 100% ━━━━━━━━━━━━ 21/21 1.6it/s 12.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.1it/s 1.8s\n",
            "                   all         41       1379      0.277      0.165      0.153     0.0731\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50      7.24G      2.403      2.347      1.597         70       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.2it/s 1.7s\n",
            "                   all         41       1379      0.577      0.159      0.169     0.0727\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50      7.24G      2.372      2.346      1.595         46       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.1it/s 1.9s\n",
            "                   all         41       1379      0.487      0.194      0.163     0.0721\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50      7.24G      2.375      2.277      1.559         53       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.1s/it 2.1s\n",
            "                   all         41       1379      0.302      0.174      0.178      0.084\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50      7.24G      2.406      2.324      1.602        140       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.556      0.168      0.192     0.0889\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50      7.24G      2.322      2.247      1.515         54       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.1s/it 2.1s\n",
            "                   all         41       1379      0.575      0.186      0.203     0.0924\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50      7.24G      2.298      2.296       1.57         37       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.2it/s 1.6s\n",
            "                   all         41       1379      0.586      0.185      0.194      0.085\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50      7.26G      2.304      2.185      1.553         53       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.1it/s 1.7s\n",
            "                   all         41       1379       0.57      0.186      0.199     0.0872\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50      7.29G      2.262      2.191      1.539         45       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.1it/s 1.9s\n",
            "                   all         41       1379      0.554      0.212      0.193     0.0854\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50       7.3G      2.302      2.213      1.554        141       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.2it/s 1.6s\n",
            "                   all         41       1379      0.639      0.164      0.208      0.081\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50       7.3G      2.217      2.132      1.504         87       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.622      0.224      0.246      0.102\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50       7.3G      2.164      2.169      1.463         20       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.2it/s 1.6s\n",
            "                   all         41       1379      0.643      0.208      0.251      0.103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50       7.3G      2.196      2.127      1.504         20       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.631      0.205       0.24       0.11\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50       7.3G      2.207      2.089      1.496         77       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.3it/s 1.5s\n",
            "                   all         41       1379      0.606      0.193      0.225     0.0918\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50       7.3G      2.194      2.061      1.492        117       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.5s\n",
            "                   all         41       1379      0.697      0.161       0.21      0.102\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50       7.3G      2.171      2.029       1.49        102       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.5s\n",
            "                   all         41       1379      0.596      0.219       0.22     0.0921\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50       7.3G      2.138      2.045      1.475         22       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.5s\n",
            "                   all         41       1379      0.648      0.209      0.248      0.128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50       7.3G      2.093      2.045      1.466         40       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.3it/s 1.6s\n",
            "                   all         41       1379      0.604      0.232      0.245      0.107\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50       7.3G      2.152      2.028      1.468         97       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.5s\n",
            "                   all         41       1379      0.644      0.251      0.267      0.115\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50      7.31G      2.108      1.966      1.449         71       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.3it/s 1.5s\n",
            "                   all         41       1379      0.592      0.239      0.258      0.128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50      7.31G      2.094      1.909      1.456         67       1024: 100% ━━━━━━━━━━━━ 21/21 1.6it/s 12.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379       0.63      0.249      0.273      0.119\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50      7.31G      2.081      1.932      1.474         64       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.631      0.256      0.283      0.132\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50      7.31G      2.101      1.927       1.43         57       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.622      0.245      0.272      0.132\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50      7.31G      2.054       1.93      1.429         52       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.5s\n",
            "                   all         41       1379      0.688      0.237      0.282      0.125\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50      7.31G      2.095      1.909      1.433         68       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.645      0.268      0.283      0.139\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50      7.31G      2.008      1.845      1.384         22       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.663       0.27      0.292      0.138\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50      7.31G      2.043       1.86      1.391        101       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.794       0.21      0.297      0.145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/50      7.31G      2.023      1.853      1.393         43       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.3it/s 1.5s\n",
            "                   all         41       1379      0.631      0.266      0.298      0.144\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/50      7.31G      2.037      1.835      1.369         74       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.635      0.272      0.286      0.142\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      41/50      7.31G      1.986      1.761      1.393         83       1024: 100% ━━━━━━━━━━━━ 21/21 1.6it/s 13.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.671      0.267      0.294      0.132\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      42/50      7.31G      1.936      1.761      1.419         63       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.695       0.26      0.296      0.142\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      43/50      7.31G      1.921      1.741      1.385         80       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.671      0.283      0.311      0.159\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      44/50      7.31G      1.897      1.751      1.387         23       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.5s\n",
            "                   all         41       1379      0.683      0.274      0.311      0.148\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      45/50      7.31G       1.88      1.708      1.393         28       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.672      0.283      0.313      0.159\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      46/50      7.31G      1.822      1.703       1.36          7       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.677      0.278      0.312      0.163\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      47/50      7.31G      1.868      1.713      1.378         20       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.691      0.279      0.314       0.16\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      48/50      7.31G      1.895      1.719      1.353         71       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.429      0.282      0.314      0.158\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      49/50      7.31G      1.872      1.684      1.365         48       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.675      0.281      0.314       0.16\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      50/50      7.31G      1.871      1.684      1.348         60       1024: 100% ━━━━━━━━━━━━ 21/21 1.7it/s 12.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.682       0.28      0.313      0.157\n",
            "\n",
            "50 epochs completed in 0.208 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup/weights/last.pt, 19.0MB\n",
            "Optimizer stripped from /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup/weights/best.pt, 19.0MB\n",
            "\n",
            "Validating /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup/weights/best.pt...\n",
            "Ultralytics 8.3.231 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLOv12s summary (fused): 159 layers, 9,232,428 parameters, 0 gradients, 21.2 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 1.4it/s 1.4s\n",
            "                   all         41       1379      0.679      0.278      0.312      0.163\n",
            "                   car         41        686      0.676      0.516      0.568      0.311\n",
            "             truck_bus         34         81      0.534       0.48      0.487      0.284\n",
            "            pedestrian         36        535      0.506      0.117       0.18     0.0519\n",
            "               cyclist         30         77          1          0     0.0151    0.00376\n",
            "Speed: 0.5ms preprocess, 29.0ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup\u001b[0m\n",
            "\n",
            "✓ Stage 1 complete\n",
            "  Results saved to: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup\n",
            "  Best weights: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup/weights/best.pt\n"
          ]
        }
      ],
      "source": [
        "#Then, we'll train\n",
        "#Use this line if you want to run both stages (warm-up and fine-tuning)\n",
        "#train()\n",
        "stage1_results = trainer.train_stage1_warmup(\n",
        "  epochs=50,\n",
        "  batch_size=16\n",
        ")\n",
        "stage1_best = Path(stage1_results.save_dir) / 'weights' / 'best.pt'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RfIVrcIhrb9Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfIVrcIhrb9Q",
        "outputId": "7ff8e79d-87fb-4660-9ff8-5c56011fa679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup/weights/best.pt\n"
          ]
        }
      ],
      "source": [
        "print(stage1_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dRlhj9cvn2kl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRlhj9cvn2kl",
        "outputId": "0ded60fd-cc3c-466c-fc9e-79ea1b7ada12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 2: FINE-TUNING (All Layers Trainable)\n",
            "================================================================================\n",
            "Loaded Stage 1 weights from: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup/weights/best.pt\n",
            "\n",
            "Starting Stage 2 training...\n",
            "  Epochs: 150\n",
            "  Batch size: 8\n",
            "  Image size: 1000\n",
            "  All layers trainable\n",
            "Ultralytics 8.3.231 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/data/dataset.yaml, degrees=15.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=150, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=0, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1000, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.001, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup/weights/best.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=stage2_finetune, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
            "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  6                  -1  2    689408  ultralytics.nn.modules.block.A2C2f           [256, 256, 2, True, 4]        \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  2   2689536  ultralytics.nn.modules.block.A2C2f           [512, 512, 2, True, 1]        \n",
            "  9                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 10             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 11                  -1  1    345856  ultralytics.nn.modules.block.A2C2f           [768, 256, 1, False, -1]      \n",
            " 12                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 13             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 14                  -1  1     95104  ultralytics.nn.modules.block.A2C2f           [512, 128, 1, False, -1]      \n",
            " 15                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 16            [-1, 11]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  1    296704  ultralytics.nn.modules.block.A2C2f           [384, 256, 1, False, -1]      \n",
            " 18                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 19             [-1, 8]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 20                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 21        [14, 17, 20]  1    820956  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
            "YOLOv12s summary: 272 layers, 9,254,684 parameters, 9,254,668 gradients, 21.5 GFLOPs\n",
            "\n",
            "Transferred 691/691 items from pretrained weights\n",
            "Freezing layer 'model.21.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "WARNING ⚠️ imgsz=[1000] must be multiple of max stride 32, updating to [1024]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.5±0.2 ms, read: 35.8±9.4 MB/s, size: 55.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/data/split_data/labels/train.cache... 322 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 322/322 400.0Kit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 3.8±4.0 ms, read: 5.4±1.9 MB/s, size: 58.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/data/split_data/labels/val.cache... 41 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 41/41 9.5Kit/s 0.0s\n",
            "Plotting labels to /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 113 weight(decay=0.0), 120 weight(decay=0.0005), 119 bias(decay=0.0)\n",
            "Image sizes 1024 train, 1024 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune\u001b[0m\n",
            "Starting training for 150 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/150      12.1G      3.137      3.955      2.063         15       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 29.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.536      0.105       0.12     0.0556\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/150      12.3G      3.134      3.921      2.013        143       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 27.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.428     0.0337     0.0369     0.0157\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      3/150      12.2G      3.032      3.359      1.871        206       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.3it/s 1.3s\n",
            "                   all         41       1379          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      4/150      12.2G      2.649      2.781      1.643         95       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.3it/s 1.3s\n",
            "                   all         41       1379          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      5/150        13G      2.481      2.502      1.555         67       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 1.8it/s 1.7s\n",
            "                   all         41       1379      0.119     0.0679     0.0447     0.0166\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      6/150      12.4G      2.415      2.434      1.545        144       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 1.5it/s 2.0s\n",
            "                   all         41       1379      0.394      0.147     0.0688     0.0275\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      7/150      12.8G      2.347       2.34      1.509        139       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.497      0.139      0.123     0.0508\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      8/150        13G      2.385      2.403      1.518         98       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.449       0.16      0.121     0.0525\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      9/150      12.5G      2.262      2.268      1.474         93       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 1.9it/s 1.6s\n",
            "                   all         41       1379      0.392      0.114     0.0565     0.0203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     10/150      13.1G      2.249      2.203      1.441         31       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 1.9it/s 1.6s\n",
            "                   all         41       1379      0.559      0.194      0.192     0.0919\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     11/150      12.4G      2.272      2.193      1.464         85       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.553      0.196      0.208     0.0897\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     12/150      12.4G      2.204      2.127      1.412         83       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379       0.59      0.193      0.201     0.0824\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     13/150      12.4G      2.219      2.128      1.459        138       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.564      0.174      0.179     0.0907\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     14/150      12.4G      2.214      2.127      1.436         43       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379       0.55      0.184      0.191     0.0822\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     15/150        13G      2.155      2.058      1.421         57       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.571      0.233       0.23      0.109\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     16/150      12.4G      2.141      2.005      1.389        174       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.596      0.205      0.213     0.0862\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     17/150      12.9G      2.139      2.026      1.395         93       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.577      0.227      0.236      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     18/150      12.4G      2.144      2.029      1.431        157       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.621      0.245      0.259       0.12\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     19/150      12.2G      2.129      1.999      1.423         89       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.623      0.229      0.262      0.125\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     20/150      12.3G      2.147      2.021      1.422         20       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.595      0.235      0.249      0.111\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     21/150      12.3G      2.121      1.994      1.418         27       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.558      0.249      0.239      0.116\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     22/150      13.2G       2.11      1.997      1.397         99       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.688      0.232      0.275      0.122\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     23/150      12.4G        2.1      2.006      1.405         17       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.535      0.269       0.24      0.113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     24/150        13G       2.03      1.921      1.378        154       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379       0.59      0.252      0.266      0.134\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     25/150      13.3G      2.053       1.91      1.373         50       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.596      0.282      0.281      0.134\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     26/150      12.4G      2.081      1.908      1.377         73       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.654      0.266       0.29      0.143\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     27/150      12.5G      2.066      1.914      1.377         85       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.598      0.283      0.291      0.144\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     28/150      12.3G      2.016      1.865      1.349         38       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.656      0.265      0.281      0.131\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     29/150      12.4G      2.072      1.874      1.376         78       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.655      0.278      0.311      0.154\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     30/150      12.6G      2.056      1.907      1.382        159       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.588       0.27      0.266      0.137\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     31/150      12.4G      2.062      1.872      1.349         95       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.611      0.287      0.291      0.153\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     32/150      12.3G      2.034      1.817      1.339         78       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.655      0.287      0.307      0.152\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     33/150      12.5G      1.992      1.774      1.328        131       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379       0.56       0.29      0.283      0.139\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     34/150      12.3G      1.984      1.788      1.333         88       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.635      0.278      0.302      0.152\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     35/150      12.6G      2.002      1.815      1.323        184       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.637      0.279      0.308      0.138\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     36/150      12.4G      1.982      1.752       1.34         91       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.628      0.291      0.312      0.139\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     37/150      12.4G      1.973      1.786      1.338         32       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.629      0.298       0.32      0.146\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     38/150      12.5G      1.955      1.748       1.34         72       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.653      0.296      0.327      0.155\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     39/150      12.4G      1.954      1.721      1.317         89       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.713      0.282      0.322      0.138\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     40/150      12.4G       1.98      1.767       1.32        125       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.622      0.303      0.304      0.165\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     41/150      12.2G      1.959      1.748       1.33         37       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.683      0.304      0.332      0.167\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     42/150      12.4G      1.917      1.694      1.319         54       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379       0.66      0.306       0.32      0.148\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     43/150      12.4G      1.965      1.719      1.336         69       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.704      0.302      0.337      0.168\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     44/150      13.2G      1.959      1.722      1.327         58       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379       0.48       0.32      0.324      0.155\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     45/150      12.5G      1.966      1.754      1.336         62       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.642      0.305      0.331      0.175\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     46/150      12.4G      1.988      1.744      1.334        247       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.627      0.308      0.317      0.147\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     47/150      12.7G      1.968      1.764      1.345          5       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379       0.69      0.319      0.348       0.19\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     48/150      12.4G      1.935      1.671      1.304        132       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.453      0.337      0.337      0.169\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     49/150      12.2G      1.916      1.693      1.317         65       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379       0.65      0.325       0.34      0.149\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     50/150        13G      1.886      1.638      1.303         87       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379       0.65      0.318      0.337      0.163\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     51/150      12.5G      1.885      1.641      1.285        126       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.655      0.346      0.368      0.179\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     52/150      12.3G      1.906      1.617       1.29         47       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.649      0.324      0.351      0.188\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     53/150      13.1G      1.963      1.721      1.327        122       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.685      0.307       0.35      0.182\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     54/150      12.2G      1.884      1.633      1.277        159       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.683      0.305       0.35      0.162\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     55/150      12.9G      1.888      1.631      1.299         51       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.729      0.314      0.368      0.179\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     56/150      12.9G      1.881      1.647      1.286         50       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.645      0.327      0.361      0.167\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     57/150      12.3G      1.893       1.62      1.277         30       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.667      0.327      0.359       0.18\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     58/150      12.2G      1.885      1.621      1.289        199       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379       0.65      0.317      0.365       0.19\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     59/150      12.3G      1.828      1.568      1.271         46       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.675      0.325      0.359      0.176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     60/150      12.4G      1.832      1.589      1.273         25       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.746      0.332      0.376      0.176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     61/150      12.4G      1.852       1.62      1.282         86       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.681      0.341      0.362      0.162\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     62/150      12.3G      1.869      1.568      1.273         68       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.718      0.343      0.381      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     63/150      12.3G      1.885      1.596      1.282         56       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.546      0.349      0.381      0.204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     64/150      12.4G      1.877      1.626      1.291         97       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.545       0.34      0.358      0.193\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     65/150      12.2G       1.87      1.588      1.291         52       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.653      0.325      0.368      0.185\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     66/150      12.3G      1.869      1.557      1.272         28       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.492       0.34      0.373      0.178\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     67/150      12.9G      1.783      1.504      1.243         81       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.611      0.359      0.379      0.185\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     68/150      12.3G      1.863       1.57      1.282         81       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.399      0.338      0.362      0.185\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     69/150      12.4G      1.855      1.592       1.29         67       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.522      0.368      0.387      0.203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     70/150      12.2G      1.771      1.476      1.248        126       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.583      0.341      0.384      0.186\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     71/150      12.3G       1.79      1.507      1.248        142       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.687      0.353      0.396      0.215\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     72/150      12.7G      1.812      1.525      1.288         90       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.519      0.361      0.379      0.197\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     73/150      12.2G      1.831       1.56      1.291         43       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.575      0.362      0.386      0.206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     74/150      12.3G      1.815      1.545      1.263         83       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.505       0.35      0.372      0.192\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     75/150      12.4G      1.801      1.519       1.27        123       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.406      0.374      0.377      0.206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     76/150      12.4G      1.783      1.509      1.272         66       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.521      0.366      0.387      0.201\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     77/150      13.1G      1.753      1.475      1.235         43       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379       0.48      0.348      0.396      0.204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     78/150      12.3G      1.792      1.473      1.259         95       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.634      0.344      0.397        0.2\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     79/150      12.4G      1.826      1.499      1.262        140       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.541      0.362      0.398      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     80/150      12.4G      1.883      1.541      1.296         85       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379       0.65      0.361      0.403      0.214\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     81/150      12.4G      1.796       1.47      1.246        243       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.625      0.366      0.399      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     82/150      12.4G      1.749      1.455      1.246         71       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.527      0.358      0.394      0.216\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     83/150      12.4G      1.779      1.479      1.246        102       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.638       0.35      0.392       0.22\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     84/150      12.3G      1.778      1.449      1.244         44       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.722      0.366      0.397      0.202\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     85/150      12.5G      1.777      1.466      1.232        102       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.672      0.375        0.4      0.207\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     86/150      12.5G      1.742      1.411      1.234        120       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.555       0.34      0.387      0.208\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     87/150      12.4G      1.794      1.456      1.243        120       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379       0.55      0.363      0.402      0.213\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     88/150      12.3G      1.748      1.416       1.21         25       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.527      0.374      0.409      0.218\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     89/150      12.2G      1.744      1.422      1.237         67       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.518      0.355      0.392      0.213\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     90/150      12.3G      1.783      1.445      1.246         59       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.723      0.372      0.407      0.221\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     91/150      12.9G      1.739      1.405      1.227        120       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.717      0.372      0.414      0.207\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     92/150      12.4G      1.711      1.406      1.228         53       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.703      0.368      0.398      0.212\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     93/150      12.7G       1.73      1.435      1.233        144       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.717      0.367      0.415      0.226\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     94/150        13G      1.734      1.413      1.228        181       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379       0.76      0.351      0.419      0.223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     95/150      12.5G      1.747      1.451      1.248        113       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.636       0.37      0.423       0.23\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     96/150      12.4G      1.705       1.37      1.215         50       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.668      0.372      0.417      0.224\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     97/150      12.4G      1.728       1.39      1.221         54       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.559      0.368      0.414      0.207\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     98/150      12.4G      1.725      1.405      1.232        161       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.546      0.376      0.405      0.229\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     99/150      12.4G      1.712      1.378       1.22         52       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.592      0.367      0.421      0.226\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    100/150      12.3G      1.724      1.411      1.219        124       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379       0.72      0.373      0.412      0.233\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    101/150      12.4G      1.727      1.415      1.246         51       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.475      0.377      0.414      0.229\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    102/150      12.5G      1.708       1.39      1.215         94       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.556      0.383      0.417      0.229\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    103/150      12.2G      1.718      1.381      1.196         67       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.587      0.367      0.414      0.227\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    104/150      12.4G      1.677      1.355      1.215        108       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.619      0.373       0.42      0.218\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    105/150      12.2G      1.683      1.352       1.22         53       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.702      0.386      0.426      0.234\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    106/150      12.3G      1.697      1.378      1.212        131       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.773      0.375      0.428      0.229\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    107/150      12.6G      1.705      1.393        1.2         46       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.473      0.377      0.422      0.226\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    108/150        13G      1.706      1.396      1.224         40       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.728      0.376      0.417      0.239\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    109/150      12.3G       1.71      1.386      1.212        124       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.646      0.395      0.428      0.237\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    110/150        12G      1.692      1.378      1.212        158       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.721      0.381      0.423      0.232\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    111/150      12.2G      1.653      1.375      1.206         31       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.567       0.38      0.426       0.23\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    112/150        13G      1.694       1.37      1.222         43       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.545      0.402      0.431      0.238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    113/150      13.1G      1.644      1.305      1.183         61       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.576      0.385      0.433      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    114/150      12.4G      1.634      1.307       1.17        130       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.675      0.371      0.431      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    115/150      13.2G      1.661      1.338      1.184         85       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.648       0.37      0.431      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    116/150      12.4G      1.679      1.351      1.202         60       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.635      0.378      0.433      0.239\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    117/150      12.3G       1.67      1.342      1.207        111       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.637      0.383      0.438      0.236\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    118/150      12.7G      1.687      1.348      1.208         35       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.675      0.379      0.431      0.239\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    119/150      12.4G      1.627      1.301      1.193         90       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.675      0.384       0.43      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    120/150      12.5G      1.648      1.332       1.18         91       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.721      0.372      0.426      0.246\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    121/150      12.6G      1.639      1.292      1.173         51       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.745      0.386      0.427      0.246\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    122/150      12.2G       1.65      1.325      1.205         72       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.695      0.385      0.432      0.238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    123/150      12.3G      1.624       1.29      1.176        163       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.579      0.393       0.43       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    124/150      12.3G      1.649      1.327      1.193        121       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.642      0.399       0.44      0.237\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    125/150        13G      1.661       1.33      1.194         59       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.622      0.391      0.434      0.245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    126/150      12.3G      1.622      1.285      1.172         51       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.641      0.396      0.436      0.245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    127/150      12.4G      1.644      1.324      1.192         94       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379       0.64      0.397      0.437      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    128/150      12.3G      1.642      1.312      1.176        249       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.722      0.395      0.431      0.246\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    129/150      12.3G      1.656       1.34      1.197         45       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.726      0.406      0.436      0.247\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    130/150      12.4G      1.658      1.315      1.185         85       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.741      0.394      0.436      0.249\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    131/150      12.5G      1.621      1.274      1.174         47       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.735      0.399      0.437       0.25\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    132/150      12.2G      1.596      1.268       1.17        101       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.592      0.406       0.44      0.247\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    133/150      12.3G      1.622      1.298      1.191         85       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.625      0.401      0.439      0.244\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    134/150      12.5G      1.628      1.302      1.192         34       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.639      0.396      0.434      0.245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    135/150      12.4G      1.633      1.304      1.189        129       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.647      0.392      0.434      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    136/150      12.4G      1.623      1.292      1.184         87       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.667      0.396      0.441      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    137/150      12.3G      1.615      1.303      1.167        163       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.662      0.394      0.439      0.244\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    138/150      12.3G      1.612      1.296      1.199         37       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379       0.61      0.401      0.435      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    139/150      13.1G      1.629      1.284      1.179        168       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.605      0.404      0.438      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    140/150      12.4G      1.648      1.299      1.192         43       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 28.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.655      0.396      0.438      0.242\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    141/150      12.3G      1.522      1.204      1.199         52       1024: 100% ━━━━━━━━━━━━ 41/41 1.4it/s 29.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.539      0.401      0.432      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    142/150      12.3G        1.5      1.172      1.184         75       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.476      0.394      0.431       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    143/150      12.2G      1.498      1.174      1.173         72       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379       0.53      0.396      0.431       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    144/150      12.2G       1.54      1.223      1.198         17       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.539      0.402      0.433      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    145/150      12.4G       1.51      1.178      1.188         77       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.541      0.399      0.432      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    146/150      12.3G      1.481      1.158      1.163         62       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.556      0.403      0.433      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    147/150      12.2G      1.491      1.181      1.181         26       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.0it/s 1.5s\n",
            "                   all         41       1379      0.537      0.401      0.434      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    148/150      12.2G      1.518      1.199      1.179         54       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.558      0.404      0.433      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    149/150      12.3G      1.506      1.204      1.188        105       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.545      0.403      0.435      0.241\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    150/150      12.3G      1.521       1.21      1.209         25       1024: 100% ━━━━━━━━━━━━ 41/41 1.5it/s 28.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.5s\n",
            "                   all         41       1379      0.529      0.403      0.435      0.242\n",
            "\n",
            "150 epochs completed in 1.293 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune/weights/last.pt, 19.0MB\n",
            "Optimizer stripped from /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune/weights/best.pt, 19.0MB\n",
            "\n",
            "Validating /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune/weights/best.pt...\n",
            "Ultralytics 8.3.231 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLOv12s summary (fused): 159 layers, 9,232,428 parameters, 0 gradients, 21.2 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 2.1it/s 1.4s\n",
            "                   all         41       1379      0.736      0.396      0.437       0.25\n",
            "                   car         41        686      0.767      0.638      0.729      0.446\n",
            "             truck_bus         34         81      0.564      0.605      0.608      0.424\n",
            "            pedestrian         36        535      0.612      0.318      0.374      0.122\n",
            "               cyclist         30         77          1     0.0229     0.0364    0.00658\n",
            "Speed: 0.5ms preprocess, 28.1ms inference, 0.0ms loss, 2.8ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune\u001b[0m\n",
            "\n",
            "✓ Stage 2 complete\n",
            "  Results saved to: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune\n",
            "  Best weights: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune/weights/best.pt\n"
          ]
        }
      ],
      "source": [
        "# Stage 2: Fine-tuning\n",
        "\n",
        "#Uncomment this line if starting over, but want to go straight to Fine-tuning\n",
        "#stage1_best = '/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage1_warmup/weights/best.pt'\n",
        "\n",
        "stage2_results = trainer.train_stage2_finetune(\n",
        "    stage1_weights_path=stage1_best,\n",
        "    epochs=150,\n",
        "    batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "v2tNiYzNcodr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2tNiYzNcodr",
        "outputId": "acdf8de5-91c6-42e6-d5ea-68bc5c3a8a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluation Stage ===\n",
            "\n",
            "Loading model from: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune/weights/best.pt\n",
            "✓ Model loaded successfully\n",
            "\n",
            "================================================================================\n",
            "MODEL EVALUATION\n",
            "================================================================================\n",
            "\n",
            "Evaluation settings:\n",
            "  Confidence threshold: 0.25\n",
            "  NMS IoU threshold: 0.45\n",
            "  Image size: 1000\n",
            "\n",
            "Running inference on test set...\n",
            "WARNING ⚠️ imgsz=[1000] must be multiple of max stride 32, updating to [1024]\n",
            "Ultralytics 8.3.231 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLOv12s summary (fused): 159 layers, 9,232,428 parameters, 0 gradients, 21.2 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 47.3±7.0 MB/s, size: 65.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/data/split_data/labels/test.cache... 41 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 41/41 80.4Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 1.6s/it 4.7s\n",
            "                   all         41       1096      0.771      0.388        0.6      0.337\n",
            "                   car         41        508      0.824      0.618      0.763      0.525\n",
            "             truck_bus         35         78       0.82      0.641      0.778      0.542\n",
            "            pedestrian         29        446       0.69      0.244      0.459      0.187\n",
            "               cyclist         23         64       0.75     0.0469      0.401     0.0938\n",
            "Speed: 9.4ms preprocess, 49.8ms inference, 0.6ms loss, 6.4ms postprocess per image\n",
            "Saving /content/runs/detect/val3/predictions.json...\n",
            "Results saved to \u001b[1m/content/runs/detect/val3\u001b[0m\n",
            "✓ Evaluation complete\n",
            "\n",
            "================================================================================\n",
            "EVALUATION METRICS\n",
            "================================================================================\n",
            "\n",
            "Overall Performance:\n",
            "  mAP@0.5:      0.6004\n",
            "  mAP@0.5:0.95: 0.3371\n",
            "  Precision:    0.7709\n",
            "  Recall:       0.3876\n",
            "\n",
            "Per-Class Performance:\n",
            "  Class           mAP@0.5    Precision    Recall    \n",
            "  --------------------------------------------------\n",
            "  Car             0.5249     0.8241       0.6181    \n",
            "  Truck/Bus       0.5425     0.8197       0.6410    \n",
            "  Pedestrian      0.1871     0.6899       0.2444    \n",
            "  Cyclist         0.0938     0.7500       0.0469    \n",
            "\n",
            "Inference Speed:\n",
            "  Preprocess:  9.40 ms\n",
            "  Inference:   49.84 ms\n",
            "  Postprocess: 6.40 ms\n",
            "  Total:       65.64 ms\n",
            "  FPS:         15.23\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Analyzing class distribution...\n",
            "\n",
            "Class Distribution:\n",
            "  Class           Count      Percentage\n",
            "  ----------------------------------------\n",
            "  Car             0            0.00%\n",
            "  Truck/Bus       0            0.00%\n",
            "  Pedestrian      0            0.00%\n",
            "  Cyclist         0            0.00%\n",
            "\n",
            "  Total: 0\n",
            "\n",
            "Generating prediction visualizations...\n",
            "✓ Saved 10 prediction visualizations to: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/results/predictions\n",
            "\n",
            "Generating performance report...\n",
            "✓ Report saved to: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/results/evaluation_report.txt\n",
            "\n",
            "✓ Pipeline complete\n",
            "mAP@0.5: 0.6004\n",
            "mAP@0.5:0.95: 0.3371\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Lastly, we'll evaluate\n",
        "\n",
        " # Results saved to: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune\n",
        " # Best weights: /content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune/weights/best.pt\n",
        "best_model_path = '/content/drive/MyDrive/Colab Notebooks/MSAAI521_FinalProject/build/runs/detect/stage2_finetune/weights/best.pt'\n",
        "\n",
        "evaluate(best_model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
